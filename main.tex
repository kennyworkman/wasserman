\documentclass[10pt]{article}
\usepackage{kennyworkman}
\usepackage{graphicx}
\usepackage{float}
\usepackage{chngcntr}

\counterwithin{exercise}{section}
\counterwithin{theorem}{section}

\title{Wasserman: All of Statistics}
\author{Kenny Workman}
\date{\today}

\begin{document}

\maketitle

\section{Basics}

\begin{definition}
    The \textbf{sample space} $\Omega$ is the set of outcomes from an experiment. Each
    point is denoted $\omega$ and subsets, eg. $A \subset \Omega$ are called
    \textbf{events}.
\end{definition}

\begin{definition}[Axioms of Probability]
    A function $\P: \Omega \to \R$ that assigns a real number to each event $A
    \subset \Omega$ is called a \textbf{probability function} or
    \textbf{probability measure} if it satsifies these three axioms:
    \begin{enumerate}
        \item \textbf{Non-negativity}. $\P(A) \ge 0$ for every event $A$
        \item \textbf{Normalization}. $\P(\Omega) = 1$.
        \item \textbf{Additivity}. $\P(A\cup B) = \P(A) + \P(B)$ if $A \cap B = \emptyset$.
    \end{enumerate}
\end{definition}

It is incredible, and not obvious, that much of probability is built up from
these only these three axioms

\begin{example}

It's actually tricky to show $\P(A \cup B) = \P(A) + \P(B) - \P(A \cap B)$ with
these three facts:

\begin{align*}
\P(A \cup B) &= \P(AB^c \cap AB \cap A^cB) \\
&= \P(AB^c) + \P(AB) + \P(A^cB) \\
&= \P(AB^c) + \P(AB) + \P(A^cB) + P(AB) - P(AB) \\
&= \P(AB^c \cup AB) + \P(A^cB \cup AB) - P(AB) \\
&= \P(A) + \P(B) - P(AB)
\end{align*}
\end{example}

Another simple idea is that events that are identical at the limit should have
identical probabilities.

\begin{theorem}[Continuity of Events]
If $A_n \to A$ then $\P(A_n) \to \P(A)$.
\end{theorem}

\begin{proof}
    Let $A_n$ be monotone increasing: $A_1 \subset A_2 \subset \dots$. Let
    $A = \lim_{n \to \infty} A_n = \bigcup_{i=1}^{\infty} A_i$.

    Construct disjoint sets $B_i$ from each $A_i$ where $B_1 = A_1$ and $B_n =
    \{ \omega \in \Omega : \omega \in A_n,\omega \notin \bigcup_{i=1}^{i-1}A_i \}$. It will be
    shown that (1) each pair of $B_i$ are disjoint, (2) $\bigcup_{i=1}^n A_i =
    \bigcup_{i=1}^n B_i$ and (3) $A = \bigcup_{i=1}^\infty A_i =
    \bigcup_{i=1}^\infty B_i$ (Exercise 1.1).

    From Axiom 3: $\P(A_n) = \P(\bigcup\limits_{i=1}^n A_i) = \P(\bigcup\limits_{i=1}^n B_i) =
    \sum\limits_{i=1}^n \P(B_i)$.

    Then $\lim_{n \to \infty} \P(A_n)
    = \lim_{n \to \infty} \sum\limits^{n}\P(B_n)
    = \sum\limits^{\infty}\P(B_n)
    = \P(\bigcup\limits^{\infty}B_n)
    = \P(A)$

\end{proof}

\begin{definition}[Conditional Probability]
    If $\P(B) > 0$, then the probability of $A$ given $B$ is 
    \[\P(A \mid B) = \dfrac{\P(AB)}{\P(B)}.\]
\end{definition}

\begin{theorem}[Total Probability]
    If $A_1 \cdots A_k$ partition $\Omega$, $\P(B) = \sum_{i=1}^k\P(B\mid A_i)\P(A_i)$
\end{theorem}

\begin{note}
    It can be difficult to assign a probability to every subset of $\Omega$. In
    practice, we only assign values to select subsets described by a \textbf{sigma
    algebra} denoted $A$. This is a subset algebra with three properties:

    \begin{itemize}
        \item{Non empty. $\emptyset \in A$. "Measure of the impossible."}
        \item{Closed over unions. $A_1, A_2 \cdots \in A \implies \cup_i A_i \in A$.}
        \item{Closed over complements. $A \in A \implies A^c \in A$.}
    \end{itemize}
    Every set in $A$ is considered \textbf{measurable} (by its membership in
    the sigma algebra). $A$ along with $\Omega$ comprises a \textbf{measurable
    space}, denoted by the pair $(A, \Omega)$. If the measure on $A$ is a
    probability function, importantly $\P(\Omega) = 1$, this space is also a
    \textbf{probability space}, denoted by the triple $(\Omega, A, \P)$.

    When $\Omega$ is the real line, the measure is often the Lebesgue measure,
    assigning intuitive values of "set length", eg. $[a, b] \mapsto b - a$.

    Why is this important? While overly pedantic at first glance, this is the
    structure that explains why continuous density functions (next section)
    have nonzero probabilities when integrated over intervals but assign 0
    probability to single points. The continuous measure, eg. Lebesgue measure,
    defined on the underlying probability space assigns positive values to sets
    and 0 to single points.
\end{note}


\begin{exercise}
    Fill in the details for Theorem 1.2 and extend to the case where $A_n$ is monotone decreasing.
\end{exercise}

\begin{proof}
    For any pair $B_{n+1}$ and $B_n$, because $B_n \subset A_n$ and $B_{n+1}
    \cap A_n = \emptyset$, it follows that $B_{n+1} \cap B_n = \emptyset$.

    Let $\bigcup_{i=1}^n B_i = \bigcup_{i=1}^n A_i$. Then $\bigcup_{i=1}^{n+1}
    B_i = (A_{n+1} \setminus \bigcup_{i=1}^n A_i) \bigcup (\bigcup_{i=1}^n
    A_i) = \bigcup_{i=1}^{n+1} A_i$.

    For the monotone decreasing case, let $A_n$ be a sequence where $A_1 \supset A_2 \supset A_3 \dots$. 

    Observe $A_1^c \subset A_2^c \dots$ and $\lim_{n \to \infty} A_n = \Omega
    \setminus \bigcup^{\infty} A_i^c$. Construct disjoint $B^c_n$ from $A^c$ in
    the same way.

    Then $\lim_{n \to \infty} \P(A_n) = 1 - \sum\limits^{\infty} \P(B^c_i) = 1 -
    \P(A^c) = \P(A)$
\end{proof}


\setcounter{exercise}{2}
\begin{exercise}
    Let $\Omega$ be a sample space and $A_1, A_2, \dots$ be events. Define
    $B_n = \cup_{i=n}^{\infty} A_i$ and $C_n = \cap_{i=n}^{\infty} A_i$.
    \begin{enumerate}[(a)]
        \item{Show $B_1 \supset B_2 \supset B_3 \dots$ and $C_1 \subset C_2
            \subset C_3 \dots$}
        \item{Show $\omega \in \cap_{n=1}^{\infty} B_n$ iff $\omega$ is in an
            infinite number of the events}
        \item{Show $\omega \in \cup{n=1}^{\infty} C_n$ iff $\omega$ belongs to all
                of the events, except possibly a finite number of those
            events.}
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[(a)]
        \item{Certainly $\cup_{i=1}^{\infty} A_i \supset \cup_{i=2}^{\infty} A_i \dots$ and $\cap_{i=1}^{\infty} A_i \subset \cap_{i=2}^{\infty} \dots$.}
        \item{Forward. Assume $\omega \in \cap_{n=1}^{\infty}B_n$. If $\omega$
            does not belong to an infinite number of events $A_i$, there exists
        some index $j$ past which $\omega \notin B_j$. Then certainly $\omega \notin \cap_{n=1}^{\infty}B_n$. Reverse. $\omega$ belonging to infinite events means there cannot exist such a $j$ described previously so $\omega \in B_n$ for all $n$. Indeed $\omega \in \cap_{n=1}^{\infty}B_n$}
    \item{Forward. Assume $\omega \in \cup_{n=1}^{\infty}C_n$. Then $\omega \in
        C_j = \cap_{i=j}^{\infty} A_i$ for some $j$. This is another way of
    saying $\omega$ is in every single event except for perhaps a finite number
in $A_{i < j}$. Reverse. Let $j$ be the index of the largest event that
$\omega$ is not in. Then $\omega \in C_{n > j}$ and certainly $\omega \in
\cup^{\infty}C_n$.}
    \end{enumerate}
\end{proof}

\begin{note}
    The key idea above is this notion of "infinitely often" (i.o.) and "all but finitely
    often" (eventually) which are two distinct structures of infinite occurence in
    sequences. Consider an $\omega$ that exists in every other event (eg. just the
    odd indices) for infinite events and revisit its inclusion in $\cap^{\infty}
    B_i$ and $\cup^{\infty} C_i$.
\end{note}

\begin{note}
    % https://chatgpt.com/c/67c5f3d7-48f4-8004-a28f-8f25f47b44c4
    $\lim \cap \cup A_n$ is also referred to as the limit infimum of $A_n$. Similarly,
    $\lim \cup \cap A_n$ is referred to as the limit supremum of $A_n$.
    %TODO: show equivalence
    % Then revisit Borel Cantelli lemma: https://en.wikipedia.org/wiki/Borel%E2%80%93Cantelli_lemma
\end{note}

\setcounter{exercise}{6}
\begin{exercise}
    Let $\P(\bigcup\limits^n A_i) \leq \sum\limits^n \P(A_i)$. Then
    $\P(A_{n+1} \cup (\bigcup\limits^n A_i)) \leq \P(A_{n+1}) +
    (\sum\limits^n \P(A_i)) - \P(A_{n+1} \cap(\bigcup\limits^n A_i)) \leq
    \sum\limits^{n+1}\P(A_i)$
\end{exercise}

\begin{note}
% https://en.wikipedia.org/wiki/Boole%27s_inequality
    Expand a bit on the Boole inequality.
\end{note}

\setcounter{exercise}{8}
\begin{exercise}
    For fixed $B$ s.t. $\P(B) > 0$, show $\P(.\mid B)$ satisfies the three
    axioms of probability.
\end{exercise}
\begin{proof}
    \begin{itemize}
        \item{\textbf{Non-negativity}. If $\P(B) > 0$ and $\P(AB) > 0$ for any
                $A \subset \Omega$, certainly
            $\frac{\P(AB)}{\P(B)} > 0$}.
        \item{\textbf{Normalization}. $\frac{\P(\Omega \cap B)}{\P(B)} =
            \frac{\P(B)}{\P(B)} = 1$}
        \item{\textbf{Additivity}. Let $AB \cap CB = \emptyset$, then $\P(AB
            \cap CB) = \P(AB) + \P(CB)$. Indeed $\frac{\P(AB\cap CB)}{B} =
        \frac{\P(AB)}{B} + \frac{\P(CB)}{B}$}
    \end{itemize}
\end{proof}

\setcounter{exercise}{10}
\begin{exercise}
    Suppose $A$ and $B$ are independent events. Show that $A^c$ and $B^c$ are
    also independent.
\end{exercise}
\begin{proof}
    We are given $\P(AB) = \P(A)\P(B)$. Then $\P(A^c)\P(B^c) = (1 - \P(A))(1
    - \P(B)) = 1 - \P(A) - \P(B) + \P(A)\P(B) = 1 - \P(A\cup B) = \P(A^cB^c)$.
    The second to last equality uses independence of $P(AB)$. The last equality
    uses the property of set complements $P(A \cup B) = P(A^c \cap B^c)$.
\end{proof}

\setcounter{exercise}{12}
\begin{exercise}
    Suppose a fair coin is tossed repeatedly until heads and tails is each
    encoutered exactly once. Describe $\Omega$ and compute the probablity
    exactly three tosses are needed.
\end{exercise}
\begin{proof}
    \begin{itemize}
        \item{The sample space is the set of binary strings with exactly one
            $0$ and $1$. For strings of length greater than 2, these are
        repeated strings of $0$ or $1$ capped with a $1$ or $0$ respectively.}
    \item{By independence, each n-string has an identical probability
        $\frac{1}{2}^n$. There are two such 3-strings: 001 and 110. Using additivity,
    $\P(\text{3 tosses}) = \frac{1}{8} + \frac{1}{8} = \frac{1}{4}$}
    \end{itemize}
\end{proof}

\setcounter{exercise}{14}
\begin{exercise}
    The probability a child has blue eyes is $\frac{1}{4}$. Assume independence
    between children. Consider a family with 3 children.
    \begin{itemize}
        \item{If it is known that at least one of the children have blue eyes,
            what is the probablity that at least two of the children have blue
        eyes?}
    \item{If it is know that the youngest child has blue eyes, what is the
        probability that at least two of the children have blue eyes?}
    \end{itemize}
\end{exercise}
\begin{proof}
    \begin{itemize}
        \item{Straightforward conditional probability. Let $A$ be the event
                where at least one child has blue eyes and $B$ be the event
                where at least two children have blue eyes. Consider first,
                $\P(A) = 1 - \P(\text{no child has blue eyes}) =
                1 - \frac{27}{64} = \frac{37}{64}$. Compute $\P(A\cap B)$ by
                enumerating events 101, 111, 110 and using additivity: $2\cdot
                \frac{1}{4}^2\cdot\frac{3}{4} + \frac{1}{4}^3 = \frac{10}{64}$. Then $\P(B\mid A) = \frac{\P(A
            \cap B)}{\P(A)} = \frac{10}{64}\cdot\frac{64}{37} = \frac{10}{37}$}
        \item{Similar procedure. Let $A$ be the event where the youngest child
            has blue eyes and $B$ be as before. Using independence, $\P(A) =
        \frac{1}{4}$. (To see this rigorously, enumerate the sample space and
    see $\P(\Omega\mid \text{first child blue}) = 1$). Now $\P(B \cap A)$
    describe events 110, 101, 111 only. $\frac{7}{64}\cdot\frac{4}{1} =
\frac{7}{16}$.}
    \end{itemize}
\end{proof}

\setcounter{exercise}{16}
\begin{exercise}
    Show $\P(ABC) = \P(A \mid BC)\P(B \mid C)\P(C)$
\end{exercise}
\begin{proof}
    By straightforward application of the definition of conditional
    probability: $\dfrac{\P(ABC)}{\P(BC)}\dfrac{\P(BC)}{\P(C)}\P(C) = \P(ABC)$
\end{proof}


\setcounter{exercise}{18}
\begin{exercise}
    Suppose $50\%$ of computer users are Windows. $30\%$ are Mac. $20\%$ are
    Linux. Suppose $65\%$ of Mac users, $82\%$ of
    Windows users and $50\%$ of Linux users get a virus. We select a person at random and learn
    they have the virus. What is the probability they are a Windows user?
\end{exercise}
\begin{proof}

    Let each $\omega \in \Omega$ be a distinct user. Then $W, M, L \subset
    \Omega$ are the users with Windows, Mac + Linux machines. $V, N \subset
    \Omega$ are the users with and without viruses.

    We want $\P(W\mid V) = \dfrac{\P(V\mid W)\P(W)}{\P(V)}$. Compute
$\P(V) = \sum_{X = \{W, M, L\}} \P(V\mid X)\P(X) = 0.705$. Then $\P(W\mid
V)= \dfrac{0.82\cdot0.50}{0.705} = 0.581$.
\end{proof}

\setcounter{exercise}{19}
\begin{exercise}

    A box contains 5 coins, each with a different probability of heads: $0,
    0.25, 0.5, 0.75, 1$. Let $C_i$ be the event with coin $i$ and $H_i$ be the
    event that heads is recovered on toss $i$.Suppose you select a coin at random and flip it.

    \begin{itemize}
    \item{What is the posterior probability $\P(C_i \mid H_1)$ for each coin?}
    \item{What is $\P(H_2 \mid H_1)$?}
    \item{Let $B_i$ be the event that the first heads is recovered on flip $i$.
        What is $\P(C_i \mid B_i) $ for each coin?}
    \end{itemize}
\end{exercise}

\begin{proof}
    \begin{itemize}
        \item{$\P(H_1) = \frac{1}{2}$. For each coin,
            $\P(C_i \mid H) = \dfrac{\P(H \mid C_i)\P(C_i)}{\P(H)}$. $\P(H)$
            can be worked out using total probability: $\sum_i \P(H |
        C_i)\P(C_i) = \frac{1}{2}$. Then eg. the posterior $\P(C_4\mid H) =
    \frac{3}{4}\cdot\frac{1}{5}\cdot\frac{2}{1} = \frac{3}{10}$.}
        \item{Note that both tosses are conditionally independent:
            $\P(H_2H_1\mid C_i) = \P(H_2\mid C_i)\P(H_1\mid C_i)$. $\P(H_2\mid
        H_1) = \dfrac{\P(H_2 H_1)}{\P(H_1)} = \dfrac{\sum_i \P(H_2 H_1 \mid C_i) \P(C_i)}{\sum_i \P(H_1 \mid C_i) \P(C_i)}.$ Because $\P(C_i)$ is
    uniform, we can simply to $\dfrac{\sum_i \P(H_2 H_1 \mid C_i)}{\sum_i \P(H_1 \mid C_i)}$. The result is $\dfrac{\sum_i p_i^2}{\sum_i p_i}$.}
    \item{Similar idea to (a).}
    \end{itemize}
\end{proof}

\begin{note}
    Important to see that independent events are not conditionally independent
    in general. Try to construct an example.
\end{note}


\section{Random Variables}
\renewcommand{\theexercise}{2.\arabic{exercise}}
\renewcommand{\thedefinition}{2.\arabic{definition}}
-- (kenny) TODO fix counters

\begin{definition}[Random Variable]
    A random variable $X$ is a function mapping the sample space to real
    numbers: $X: \Omega \to \R$.
\end{definition}

It is important to think of the relationship between the random variable and
its underlying sample space when computing probabilities: eg. $\P(X = x) =
\P(X^{-1}(x))$ and $\P(X \in A) = \P(X^{-1}(A))$.

\begin{definition}[Cumulative Distribution Function]
    The CDF is the function $F_X: \R \to [0, 1]$ where $F_X(x) = \P(X \leq x)$.
    Equivalently $F_X(x) = \P(X^{-1}((-\infty, x])$.
\end{definition}

The CDF contains "all the information" in a random variable. This is
articulated by the following theorem:

\begin{theorem}
    For random variables $X$ and $Y$ with CDFs $F$ and $G$, if $F(x) = G(X)
    \forall x \in [0, 1]$, then $X = Y$ ($\P(X \in A) = \P(Y \in A)$ for each
    $A \subset \R$).
\end{theorem}

And the behavior of the CDF, including "all of its information" is uniquely
determined by just three properties:

\begin{theorem}
    A function $F: \R \to [0, 1]$ is a CDF iff it satisfies three properties:
    \begin{itemize}
        \item{Non-decreasing. $x_2 > x_1 \implies F(x_2) \geq F(x_1)$}
        \item{Normalization. $\lim_{y\to 0}F(y) = 0$ and $\lim_{y\to 1}F(y) = 1$}
        \item{Right-continuous. For any $x \in \R$, $F(x) = F^+(x)$ where
            $F^+(x) = \lim_{y \to x, y>x}F(y)$}
    \end{itemize}
\end{theorem}

\begin{proof}
    Starting with (iii) from the text, let $A = (-\infty, x]$ and $y_1, y_2, \dots$ be a sequence where
        $y_1 < y_2 \dots$ and $\lim_i y_i = x$. By the definition of the
        CDF, $F(y_i) = \P(A_i)$ and $F(x) = \P(A)$, where $\lim_i F(y_i)$ is equivalent to
        $\lim_{y\to x, y>x}F(y)$. Observe $\cap_i A_i = A$ so $\P(A) =
        \P(\cap_i A_i) = \lim_i \P(A_i) = \lim_i F(y_i) = F(x)$ as desired.

        To see (ii), $\lim_{y\to -\infty}F(y) = 0$, define a sequence $y_1, y_2
        \cdots$ where $y_1 > y_2 \cdots$ as before and $y_1 = y$. Let $A_i = (\infty, y_i]$.
        Then $\cap_i A_i = \emptyset$ and $\P(\cap_i A_i) = \P(\emptyset) = 0$.
        Indeed $\lim_{y \to -\infty}F(y) = \lim_{i}\P(A_i) = \P(\cap_i A_i) =
        0$. A similar argument shows the limit to the other direction.

        For (iii), if $x_2 > x_1$ then $P((-\infty, x_2]) \geq P((-\infty,
        x_1])$ and $F(x_2) \geq F(x_1)$.
\end{proof}

The interesting direction is the reverse: a function satisfying these
properties uniquely determines a probability function. It is difficult to show
in general. A concrete example is the Cantor function
(\href{https://en.wikipedia.org/wiki/Cantor_function}{Devil's staircase}) which
satisfies non-decreasing, normality and right-continuous properties but from
which is difficult to derive a measure that satisfies eg. countable additivity.

\begin{note}
    A deeper measure theory course will approach this problem by defining the
    probablity function on an algebra of subsets rather than on each subset
    directly. Refer to tools like \href{https://en.wikipedia.org/wiki/Carath%C3%A9odory%27s_extension_theorem}{Caratheodory's extension theorem}.
\end{note}

It is from these random variables that we build "distributions", essentially
functions $\R \to [0, 1]$ that obey the three probability axioms.

\begin{definition}
    If $X$ "takes" countably many values (eg. has a countable range) it is
    \textbf{discrete}. $f_X(x) = \P(X = x)$ is its \textbf{probability mass
    function} or PMF.
\end{definition}

\begin{definition}
    $X$ is \textbf{continuous} if it has some $f_X$ that obeys three properties:
    \begin{itemize}
        \item{$\int_{-\infty}^{\infty} f_X(x) \dd x = 1$}
        \item{$ \forall x \in \R: f_X(x) \geq 0$}
        \item{$\P(a < X < b) = \int_a^b f_X(x) \dd x$}
    \end{itemize}
    $f_X$ is called the \textbf{probability density function} or PDF.
    Additionally, $F_X(x) = \int_{-\infty}^x f_X(x) \dd x$ and $f_X(x) =
    F'_X(x)$ for all points $x$ where $F_X$ is differentiable.
\end{definition}

The formal relation between the density function and the sample space is a bit
tricky, especially when $X$ is continuous. In practice, we often just produce
a function and deal with it directly while assuming the underlying sample space
with a well defined measure is lurking around.

\begin{note}
    We learned the probability function is defined on a well-defined sample space by measuring events / sets. 
\end{note}

\begin{definition}
    The quartile function (or inverse CDF) is $F^{-1}(q) = \inf\{ x : q < F(x) \}$
\end{definition}

We call $F^{-1}(\frac{1/4})$ the first quartile, $F^{-1}(\frac{1/2})$ the
second quartile (or median), etc.

We will proceed with some important mass functions.

\begin{definition}[The Point Mass Distribution]
    If $X \sim \sigma_a$ (reads "$X$ has a point mass distribution at $a$"),
    $f_X(a) = 1$ while $f_X(x) = 0$ for all $x \neq a$.
    \[
    F_X(x) =
    \begin{cases}
    0, & x < a, \\
    1, & x \geq a
    \end{cases}
    \]
    \end{definition}

\begin{definition}[The Uniform Distribution]
    Suppose $X$ has a mass function:
    \[f(x) = 
    \begin{cases}
        \frac{1}{k}, & x \in \{1 \dots k\} \\
        0, & \text{o.w.}
    \end{cases}
    \]
    $X$ then has a uniform distribution on $\{1 \dots k \}$.
\end{definition}

\begin{definition}[The Bernoulli Distribution]
    If $X \sim Bernoulli(p)$, the PMF of $X$ is $f(x) = p^x(1-p)^{1-x}$ for $x
    \in \{0, 1\}$ and $p \in [0, 1]$.
\end{definition}

Here is the first instance of a parameterized random variable.

\begin{definition}[The Binomial Distribution]
    Binomial variables model the number of successful flips for $n$ identical
    trials with probability $p$ for each.
    We say $X \sim Binomial(n, p)$ with PMF:
    \[f(x) = \begin{cases}
        \binom{n}{x}p^x(1-p)^{n-x} & x \in \{1 \dots n\} \\
        0 & \text{o.w.}
    \end{cases}
    \]
\end{definition}

The following represent different ideas of unbounded "counting": trials until
success and trials in some interval of time.

\begin{definition}[The Geometric Distribution]
    Here we have the idea of flipping a coin until our first success.
    $X \sim Geometric(p)$ with PMF: $f(x) = (1-p)^{x-1}p$
\end{definition}

The probability value of each term is a geometric series. Indeed $p
\sum^{\infty}(1-p)^x = \frac{p}{1-(1-p)} = 1$.

\begin{definition}[The Poisson Distribution]
    If $X \sim Poisson(\lambda)$ with PMF $f(x) = e^{-\lambda}\frac{\lambda^x}{x!}$
\end{definition}

$\lambda$ can be thought of as some interval of time. $X$ then measures the
number of events in this interval: decaying particles or mRNA translation.

Similarly to the geometric distribution, each term in the poisson is a Taylor
polynomial, derived from the power series expansion of the exponential
function. Indeed $e^{-\lambda}\sum^{\infty}\frac{\lambda^x}{x!} = e^{-\lambda}e^{\lambda}
= 1$. 

\begin{note}
    For distributions that count trials in some interval - some time or
    number of trials - the sum of variables equals a single variable that
    accumulates the interval. 

    If $X_1 \sim Binomial(n_1, p)$ and $X_2 \sim Binomial(n_2, p)$, then $X_1 +
    X_2 \sim Binomial(n_1 + n_2, p)$.

    If $X_1 \sim Poisson(\lambda_1)$ and $X_2 \sim Poisson(\lambda_2)$, then $X_1 +
    X_2 \sim Poisson(\lambda_1 + \lambda_2)$.
\end{note}

\begin{note}
    Recall $\Omega$ really lurking around. Eg. let $X \sim Bernoulli$ and $\P(X
    = 1)$ is $\P(\omega \in [0, p]) = p$. 
\end{note}

For the continuous distributions, useful to think of integration.

\begin{definition}[The Continuous Uniform Distribution]
    If $X$ has a uniform distribution on the interval $[a, b]$ with PDF: \[f(x) = \begin{cases}
        \frac{1}{b-a} & x \in [a, b] \\
        0 & \text{o.w.}
        \end{cases}
    \]
    and CDF: \[F(x) = \begin{cases}
        0 & x < a \\
        \frac{x - a}{b - a} & x \in [a, b] \\
        1 & x \geq b \\
    \end{cases}
    \]
\end{definition}

\begin{definition}[The Normal (Gaussian) Distribution]
    $f(x) = \frac{1}{\sigma \sqrt{2\pi}}e^{-\frac{1}{2\sigma^2}(x-\mu)^2}$
\end{definition}

\begin{note}
    If $X \sim N(0, 1)$ we say that $X$ has a \textbf{standard Normal
    distribution}. We often denote $X$ as $Z$ with $\phi$ and $\Phi$ as the
    PDF and CDF.

    There is no closed form function for $\Phi$, so we use precomputed values
    from tables or rely on statistical programs. Calculations with Normal
    distributions then proceed by reexpressing $X$ as some function of $Z$ and
    using these values.
\end{note}

The following facts are essential when manipulating these variables:

\begin{itemize}
\item{If $X \sim N(\mu, \sigma)$, then $Z = \frac{X - \mu}{\sigma} \sim N(0, 1)$}
\item{If $Z \sim N(0, 1)$, then $X = \mu + \sigma Z \sim N(\mu, \sigma)$}
\item{If $X_i \sim N(\mu_i, \sigma_i)$ are independent, then $X = \sum_i X_i \sim N(\sum_i \mu, \sum_i \sigma)$}
\end{itemize}

\begin{definition}[The Exponential Distribution]
    If $X \sim Exp(\beta)$, then $f(x) = \frac{1}{\beta}e^{-\frac{x}{\beta}}$.
\end{definition}

Indeed $\int$

\begin{note}[The Gamma Function]
    We often want a continuous extension of the factorial to real
    arguments, where $\Gamma(x) = (x-1)!$ for $x \in \Z^+$. This is the
    \textbf{gamma function} and defined $\Gamma(x) = \int_0^{\infty}
    y^{x-1}e^{-y}dy$.

    Evaluating the integral for $\Gamma(1), \Gamma(2) \dots$ is a useful
    exercise to convince oneself of agreement with the factorial.

    For example, $\Gamma(3) = \int_0^{\infty} y^2e^{-y} dy$. Using integration
    by parts, this evaluates to $[-y^2e^{-y} - 2ye^{-y} - 2e^{-y}]^{\infty}_0$.
    Using L'Hopital's, the first two terms drop out and we are left with
    $\Gamma(3) = 2 = (3-1)!$ as desired.
\end{note}

Equipped with the gamma function, we can now develop the gamma distribution.

\begin{definition}[Gamma Distribution]
  Let $\alpha,\beta>0$. A continuous random variable $X$ is said to have a
  \emph{Gamma distribution} with shape parameter $\alpha$ and scale parameter
  $\beta$, denoted
  \[
    X \sim \Gamma(\alpha, \beta),
  \]
  if its probability density function is
  \[
    f_X(x)
    = \frac{1}{\beta^{\alpha}\,\Gamma(\alpha)} \; x^{\alpha - 1} \, e^{-\,x/\beta},
    \quad x \ge 0.
  \]

  If $X_i \sim \Gamma(\alpha_i, \beta)$ are independent, $\sum_i X_i =
  \Gamma(\sum \alpha_i, \beta)$.
\end{definition}

The exponential distribution is then just a special case of a gamma
distribution with $\alpha = 1$.

\begin{note}
The Gammaâ€normalization comes from evaluating
\[
  \int_{0}^{\infty} x^{\alpha-1} \,e^{-x/\beta}\,dx.
\]
We make the substitution
\[
  x = \beta\,t,\quad dx = \beta\,dt,
\]
so that
\[
  \int_{0}^{\infty} x^{\alpha-1} e^{-x/\beta}\,dx
  = \int_{0}^{\infty} (\beta t)^{\alpha-1}\,e^{-t}\,(\beta\,dt)
  = \beta^{\alpha-1}\,\beta
    \int_{0}^{\infty} t^{\alpha-1} e^{-t}\,dt
  = \beta^{\alpha}\,\Gamma(\alpha).
\]
Hence in the density
\[
  f(x) = \frac{1}{\beta^\alpha\,\Gamma(\alpha)}\;x^{\alpha-1}\,e^{-x/\beta}
\]
the factor \(\beta^\alpha\,\Gamma(\alpha)\) is exactly the normalizing constant
that makes \(\int_0^\infty f(x)\,dx=1\).  
\end{note}

\begin{definition}[$\Chi^2$ Distribution]
    $X$ has a $\Chi^2$ distribution with $p$ degrees of freedom if the PDF is
    \[
        f(x) =
        \frac{1}{\Gamma(\frac{p}{2})2^{\frac{p}{2}}}x^{\frac{p}{2}-1}e^{\frac{x}{2}}
    \]
    Let $p>0$. A random variable $X$ is said to have a $\chi^2$ distribution with 
    $p$ degrees of freedom, denoted $X \sim \chi^2_p$, if its probability density 
    function is
    \[
      f_X(x)
      = \frac{1}{2^{\,p/2}\,\Gamma\bigl(\tfrac{p}{2}\bigr)}\,
        x^{\,\tfrac{p}{2}-1}\,e^{-x/2},
      \quad x > 0.
    \]
    This distribution is the sum of squared, independent normals. If $Z_i \sim
    N(0, 1)$ then $\sum_i Z_i^2 \sim \Chi_p^2$.
\end{definition}

%% TODO: Transformations of random variables. Why F'_X = f_X.

\begin{definition}[Independence of Random Variables]
    If $\P(X \in A, Y \in B) = \P(X \in A)\P(Y \in B)$, we say $X$ and $Y$ are
    independent, written $X \indep Y$.
\end{definition}

\setcounter{exercise}{4}
\begin{exercise}
    Let $X$ and $Y$ be discrete random variables. Show $X$ and $Y$ are independent iff $f_{X,Y}(x, y) = f_X(x)f_Y(y)$
\end{exercise}

\begin{proof}
    If $\P(X \in A, Y \in B) = \P(X \in A)\P(Y \in B)$ for every subset $A, B$, let $A = \{x\}$ and $B = \{y\}$ for every possible pair of elements. Then $f_{X, Y}(x, y) =
    f_X(x)f_Y(y)$. To see the reverse, 
    $\P(X \in A, Y \in B) = \sum_{x\in A}\sum_{y\in B}f_{X,Y}(x, y) =
    \sum_{x\in A} f_X(x) \sum_{y\in B} f_Y(y) = \P(X \in A)\P(Y \in B)$
\end{proof}

\begin{theorem}
    Suppose the range of $X$ and $Y$ is a (potentially infinite) rectangle. If we can express $f_{X,Y} =
    g(x)h(y)$, then $X$ and $Y$ are independent.
\end{theorem}

\begin{proof}
    Start by computing the marginals. $f_X = \int g(x)h(y) dy = g(x) (\int h(y)
    dy)$ and $f_Y = \int g(x)h(y) dx = h(y)(\int g(x) dx)$. 

    Then $f_X f_Y = g(x) (\int h(y) dy) h(y) (\int g(x) dx) = g(x)h(y) (\int \int h(y) g(x) dx
    dy)$. Because $g(x)h(y) = f_{X,Y}$, the integration term evaluates to $1$.
    Then $f_Xf_Y=g(x)h(y)(\int \int h(y) g(x) dx dy)=g(x)h(y)(1)=f_{X,Y}$ which is exactly the condition for
    independence of $X$ and $Y$.
\end{proof}

In the above problem, notice the significance of requiring the range to be a
rectangle. Any other region would produce integration limits in one variable
that are functions of the other variable and you can no longer pull out the
integration terms from the maringals:

\[
f_X(x) \;=\; g(x)\Bigl[\underbrace{\int_{y = x^2}^{1} h(y)\,dy}_{\text{a function of }x}\Bigr].
\]

\begin{definition}[Transformation of Continuous R.V.]
    When $Y$ and $X$ are continuous.
    \begin{itemize}
        \item{Find $A_y = \{ x : r(x) \leq y \}$ for each $y \in R$ }
        \item{Then $F_Y(y) = \P(r(X) \leq y) = \int_{A_y} f_X dx$}
        \item{$f_Y = F'_Y$}
    \end{itemize}
\end{definition}

\setcounter{exercise}{0}
\begin{exercise}
    Show $\P(X = x) = F(x^+) - F(x^-)$
\end{exercise}

\begin{proof}
    The key here is to see $\lim_{z < x, z\to x}F(z) = \P(X \in \cup_i (\infty,
    z_i]) = \P(X < x)$ for some sequence $z_1, z_2, \cdots $ where $\lim_i z_i
    = x_i$.
    While $\lim_{y > x, y\to x}F(y) = \P(X \in \cap_i(\infty, y_i]) = \P(X \leq
    x)$.

    Pay attention to the behavior of converging sets and the boundary. In the
    right-continous case, the sequence is approaching the boundary $x$ from
    above and each sequence is closed on $x$. Therefore in the limit, they
    include $x$.

    In the left-continuous case, the sequence is approaching the boundary $x$
    from below and each sequence excludes $x$. Therefore in the limit, they
    exclude $x$.

    To conclude $F(x^+) - F(x^-) = \P(X \leq x ) - \P(X < x) = \P(X = x)$. Of
    course, if $X$ is continuous, $F(x^+) = F(x^-)$ and $\P(X = x) = 0$,
    showing once again that every real value has no probability mass.
\end{proof}

\setcounter{exercise}{3}
\begin{exercise}
    Let $X$ have density
    \[
    f_X(x) =
    \begin{cases}
    \frac{1}{4}, & 0 < x < 1, \\
    \frac{3}{8}, & 3 < x < 5, \\
    0, & o.w.
    \end{cases}
    \]
    \begin{itemize}
    \item{Find the CDF of $f_X$}
    \item{Let $Y = \frac{1}{X}$. Find $f_Y$.}
    \end{itemize}
\end{exercise}

\begin{proof}
    \begin{itemize}
    \begin{item}\[
    F_X(x) =
    \begin{cases}
    \frac{1}{4}x, & 0 < x < 1, \\
    \frac{1}{4}, & 1 < x < 3, \\
    \frac{3}{8}(x -3) + \frac{1}{4}, & 3 < x < 5 \\
    1, & x \geq 5
    \end{cases}
    \]
    \end{item}
    \begin{item}\[
    F_Y(y) =
    \begin{cases}
        0, & y < \frac{1}{5}, \\
        \frac{3}{8}(5-\frac{1}{y}), & \frac{1}{5} < y < \frac{1}{3}, \\
        \frac{6}{8}, & \frac{1}{3} < y < 1, \\
        \frac{1}{4}(1 - \frac{1}{y}) + \frac{6}{8}, & 1 < y, \\
    \end{cases}
    \]

    Then to compute $f_Y(y) = F_Y'(y)$, we differentiate:
    \[
    \begin{cases}
        0, & y < \frac{1}{5}, \\
        \frac{3}{8y^2}, & \frac{1}{5} < y < \frac{1}{3}, \\
        0, & \frac{1}{3} < y < 1, \\
        \frac{1}{4y^2}, & 1 < y, \\
    \end{cases}
    \]
    \end{item}
\end{itemize}
\end{proof}

\setcounter{exercise}{6}
\begin{exercise}
    Let $X$ and $Y$ be independent and suppose each is $Uniform(0, 1)$. Let 
    $Z = min\{X, Y\}$. Find the density $f_Z(z)$.
\end{exercise}

\begin{proof}
    $\P(Z > z) = \P(X > z, Y > z) = \P(X > z)\P(Y > z)$.
    Then, $\P(Z > z) = (1 - z)^2$ and $F_Z = 1 - \P(Z > z) = 1 - (1 - z)^2$.
    $f_Z = F'_Z = -2z + 2$
\end{proof}

\setcounter{exercise}{8}
\begin{exercise}
    Let $X \sim Exp(\beta)$. Find $F(x)$ and $F^{-1}(q)$.
\end{exercise}

\begin{proof}
    $f(x) = \frac{1}{\beta}e^{1/\beta}$. So $F(x) = \int_{0}^{x} f(x) dx
    = 1 - e^{-x/\beta}$.

    $F$ is a bijection over the interval $[0, \infty)$ so we can find a genuine
    inverse $F^{-1}$ as $-\beta\ln(1-q)$.

\end{proof}

Plugging in a few numbers to get a feel for $F^{-1}(q)$, we see that
$F^{-1}(0.99) = \beta 4.6$ and $F^{-1}(0.9999) = \beta 9.2$, confirming
that linear changes in sample space value have exponential effect in
probability and that eg. increasing $\beta$ decreases likelihood of events by
stretching the density.


\setcounter{exercise}{10}
\begin{exercise}
    Flip a coin once with probability heads of $p$. Let $X$ and $Y$ be the
    number of heads and tails.
    \begin{itemize}
        \item{Show $X$ and $Y$ are independent}
        \item{Let $N \sim Poisson(\lambda)$ be the number of coin flips. Show
            now that $X$ and $Y$ are independent}
    \end{itemize}
\end{exercise}

\begin{proof}
\textbf{(a) One toss.}
Because \(Y = 1 - X\),
\[
\P\{Y=1\mid X=1\}=0 \neq \P\{Y=1\}=1-p,
\]
so \(X\) and \(Y\) are dependent.

\medskip
\textbf{(b) Random number \(N\sim\mathrm{Poisson}(\lambda)\).}

\emph{Step 1 (conditional pmf).}  Given \(N=k\),
\[
\P\{X=x,\,Y=y \mid N=k\}
   =\mathbf 1_{\{x+y=k\}}
     \binom{k}{x}p^{x}(1-p)^{y}.
\]

\emph{Step 2 (unconditional pmf).}  Summing over \(k\),
only the term \(k=x+y\) remains:
\[
\P\{X=x,\,Y=y\}
  =e^{-\lambda}\frac{\lambda^{x+y}}{(x+y)!}
    \binom{x+y}{x}p^{x}(1-p)^{y}
  =e^{-\lambda}\frac{(\lambda p)^{x}}{x!}\,
     e^{-\lambda}\frac{\bigl(\lambda(1-p)\bigr)^{y}}{y!}.
\]

\emph{Step 3 (marginals).}
Hence
\[
X\sim\mathrm{Poisson}(\lambda p),\qquad
Y\sim\mathrm{Poisson}\bigl(\lambda(1-p)\bigr),
\]
and
\(\P\{X=x,\,Y=y\}=\P\{X=x\}\P\{Y=y\}\), so \(X\) and \(Y\) are independent.
\end{proof}

\setcounter{exercise}{12}
\begin{exercise}
    Let $X \sim Normal(0, 1)$ and $Y = e^X$.
    \begin{itemize}
        \item{Find $f_Y$ and plot it.}
        \item{Generate 10,000 random draws from $X$. Create a histogram of
            these draws and compare to the density plot.}
    \end{itemize}
\end{exercise}

\begin{proof}
    Because $r = e^x$ is a strictly monotonically increasing function, we can
    apply $f_Y = f_X(s(x))s'(x)$ where $s = r^{-1}$. Then $f_Y(y) =
    f_X(ln(y))\frac{1}{y}$. Using the standard normal density, $f_Y(y) =
    \frac{1}{\sqrt{2\pi}y}e^{-\frac{(\ln y)^2}{2}}$
\end{proof}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\linewidth]{code/2.13.pdf}
  \caption{Histogram of $Y=e^{X}$ overlaid with its log-normal density.}
  \label{fig:lognormal}
\end{figure}

\begin{note}
    It is worth understanding why $f_Y = f_X(s(x))s'(x)$ can be used when $r$
    is a strict monotonically increasing or decreasing function. This condition
    forces $s$ to be differentiable and single-valued for the single-variable
    change-of-variable integration. 
\end{note}

\setcounter{exercise}{14}
\begin{exercise}
    \begin{itemize}
        \item{Let $X$ have a continuous, strictly increasing CDF $F$. Let $Y =
            F^{-1}(X)$. Find the density of $Y$.}
        \item{Now let $U \sim Uniform(0, 1)$. Let $X = F^{-1}(U)$, where $F$ is
                no longer the CDF of $X$ but is still continuous and strictly
                increasing. Show $F_X = X$.}
        \item{Write a program to generate $Exponential(\beta)$ random variables
            from $Uniform(0, 1)$}
    \end{itemize}
\end{exercise}

\begin{proof}
    \begin{itemize}
        \item{$\P(Y \leq y) = \P(F(X) \leq y) = \P(X \leq F^{-1}(y)) = F(F^{-1}(y))$ So $F_y = 1$.}
        \item{$\P(X \leq x) = \P(F^{-1}(U) \leq x) = \P(U \leq F(x)) = F_U(F(x)) = F(x)$}
        \item{Using the fact that $X = F^{-1}(U)$ has CDF $F$, we compute the
                exponential CDF and find its inverse: $F_X^{-1}(q) = -\beta \ln
            (1 - \beta^2 q)$. A histogram of generated values, overlayed
        against the exponential PDF, can be found below.}
    \end{itemize}
\end{proof}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\linewidth]{code/2.15.pdf}
  \caption{Histogram of generated exponentials overlayed against theoretical PDF}
  \label{fig:generated}
\end{figure}

\setcounter{exercise}{15}
\begin{exercise}
    Let $X \sim Poisson(\lambda)$ and $Y \sim Poisson(\mu)$ be independent
    random variables. Find the density of $X$ given $X + Y = n$. Use the fact
    that $X + Y \sim Poisson(\lambda + \mu)$ and $\P(X = x, X + Y = n) = \P(X =
    x, Y = n - x)$.
\end{exercise}

\begin{proof}
    We are interested in the quantity $\P(X = x, X + Y = n | X + Y = n)$.
    Observe $\P(X + Y = n) = e^{-(\lambda + \mu)} \frac{(\lambda +
    \mu)^n}{n!}$. And $\P(X = x, X + Y = n) = \P(X = x, Y = n - x) = \P(X =
    x)\P(Y = n - x) = e^{-\lambda}\frac{\lambda^x}{x!}e^{-\mu}\frac{\mu^{n-x}}{(n-x)!}$.

    Our conditional distribution is then the expression: 

    \[ \frac{e^{-\lambda} e^{-\mu} \frac{\lambda^x}{x!} \frac{\mu^{n-x}}{(n-x)!}}{e^{-(\lambda+ \mu)} \frac{(\lambda+ \mu)^n}{n!}} \]

    Simplifying we begin to see the shape of the binomial:

    \[ \frac{\lambda^x}{x!}\frac{\mu^{n-x}}{(n-x)!} \frac{n!}{(\lambda + \mu)^n} \]
    \[ \frac{n!}{(n-x)!x!} \frac{\lambda^x\mu^{n-x}}{(\lambda + \mu)^n} \]
    \[ \frac{n!}{(n-x)!x!} \frac{\lambda^x\mu^{n-x}}{(\lambda + \mu)^{n-x} (\lambda + \mu)^x} \]

    This is $\binom{n}{x}(\frac{\lambda}{\lambda + \mu})^x (\frac{\mu}{\lambda + \mu})^(n-x)$ or $Binomial(n, \frac{\lambda}{\lambda + \mu})$.
\end{proof}

\setcounter{exercise}{19}
\begin{exercise}
\end{exercise}


\section{Expectation}

\subsection{Expectation of a Random Variable}

\begin{definition}
    The expected value (or mean or first moment) of $X$ is defined as 
    \[\E X =
    \int x dF_X(x) = 
    \begin{cases} 
        \sum_x x f(x),&\text{if X is discrete}\\ 
        \int_x x f(x)dx,&\text{if X is continuous}\\ 
    \end{cases}\]

    Assuming the sum or integral is well-defined, we use the following notation
    to denote the expected value of $X$: $\E X = \mu = \mu_X$
\end{definition}

\subsection{Properties of Expectation}

\begin{theorem}
    $\E {\sum_i X_i} = \sum_i \E X_i$
\end{theorem}

\begin{theorem}
    If $X_1 \dots X_i$ are independent, $\E {\prod X_i} = \prod_i \E X_i$
\end{theorem}

\begin{note}
    Work out the above briefly and note why we need
    independence for the product and not the sum. Eg. $\int (x + y)f_{X, Y}$
    vs. $\int xy f_{X, Y}$. Integration is additive but factorization of the
    joint PDF/PMF is necessary for the product terms.
\end{note}

\subsection{Variance and Covariance}

\begin{definition}
    The variance of $X$ is defined as 
    \[
        \E {(X - \E X)^2}
    \]
    and is denoted as $\sigma^2_X$ or $\sigma^2$ or $\Var X$
\end{definition}

\begin{theorem}
    Assuming the variance of $X$ is well-defined, it has the following
    properties:
    \begin{itemize}
        \item{$\Var X = \E{X^2} - \E{X}^2$}
        \item{$\Var {aX + b} = a^2 \Var X$}
        \item{If $X_1 \dots X_n$ are independent, $\Var{\sum_i a_i X_i} = \sum_i a_i^2 \Var{X_i}$}
    \end{itemize}
\end{theorem}

\begin{proof}
    \begin{itemize}
        \item{$\E{(X - \mu)^2} = \E{X^2 - 2 X \mu + \mu^2} = \E{X^2} - \mu^2$}
        \item{$\E {(aX + b) - \E{aX +b}}^2 = \E {(aX + b - a \mu + b)^2} = \E
            {(a(X - \mu))^2} = a^2 \E{(X - \mu)^2} = a^2 \Var X$}
        \item{$\Var {\sum_i a_i X_i} = \E {(\sum_i a_i X_i - \E {\sum_i a_i
            X_i})^2}$. Using additivity of expectation, $\E {\sum_i a_i X_i} =
        \sum_i a_i \E X_i$. Then our expression becomes $\E{ (\sum a_i X_i -
    a_i \E X_i)^2}$. Expanding this expression, we arrive at $\E {\sum_i
(a_iX_i - a_i\E X_i)^2 + \sum_{i, j} a_ia_j(X_i - \E X_i)(X_j - \E X_j)}$. The
first set of terms become $\sum_i a_i^2 \Var X_i$ and the second set of terms
drop out when expanded as every pair of variables are independent. ($\E{X_iX_j}
- \E X_i \E X_j = 0$).}
    \end{itemize}
\end{proof}

\begin{definition}
    Let $X_1 \dots X_n$ be random variables. The \textbf{sample mean} is then 
    \[\bar{X_n} = \frac{1}{n}\sum_i X_i\]
    And the \textbf{sample variance} is
    \[S^2_n = \frac{1}{n-1}\sum_i (X_i - \bar{X_n})^2\]
\end{definition}

\begin{theorem}
    If $X_1 \dots X_n$ are i.i.d. and $\E {X_i} = \mu$ and $\Var {X_i} = \sigma^2$,
    then $\E {\bar{X_n}} = \mu$, $\Var {\bar{X_n}} = \frac{\sigma^2}{n}$, and
    $\E {S^2_n} = \sigma^2$.
\end{theorem}

\begin{proof}
    \[\E{\bar{X_n}} = \frac{1}{n}\sum_i \E{X_i} = \mu\]
    \[\Var{\bar{X_n}} = \frac{1}{n^2}\sum_i \Var{X_i} = \frac{\sigma^2}{n}\]
    Notice $\sum_i (X_i - \bar{X_n})^2 = \sum_i (X_i^2 - 2 X_i \bar{X_n} +
    \bar{X_n}) = \sum_i (X_i^2) - 2 \sum_i {X_i \bar{X_n} + \sum_i
    \bar{X_n}^2}$. The inner term becomes $2 \bar{X_n} n \bar{X_n} =
    2n\bar{X_n}^2$. So:

    \[\E{S^2_n} = \E{\frac{1}{n-1} \sum_i (X_i - \bar{X_n})^2} = \frac{1}{n-1}
        \sum_i \E {X_i^2} - \E{\bar{X_n}^2} = \frac{1}{n-1}n[(\sigma^2 - \mu^2) -
        (\frac{\sigma^2}{n} - \mu^2)] = \sigma^2 \]
\end{proof}

\begin{note}
So what's up with the $\frac{1}{n-1}$?

Natural way to introduce "degrees of freedom". Consider the vector of residuals
$r_i = X_i - \bar{X_n}$. We actually "use up" one of these residuals in the following
way. 

Notice the sum of our residuals evaluates to 0. 

\[\sum_{i=1}^n (X_i - \bar{X_n}) = \sum_{i=1}^n (X_i - \frac{1}{n} \sum_{i=1}^n
X_i) = \sum_{i=1}^n X_i - \sum_{i=1}^n X_i = 0\]

This is just algebra and comes from the fact that our mean is not the true
mean rather estimated from data. So after picking $n-1$ such $r_i$, the last
$r_n$ must equal $-\sum_{i=0}^{n-1} r_i$ for this identity to hold. 

We then say that the sum of residuals, $\sum_i r_i$, used within the $S_n^2$
statistic has only $n-1$ "degrees of freedom". It is common shorthand to also
say the variance estimate itself ($S_n^2$) also has $n-1$ degrees of freedom.
\end{note}

\begin{definition}
    Let $X$ and $Y$ be r.v.s with means $\mu_X, \mu_Y$ and standard deviations
    $\sigma_X, \sigma_Y$. The \textbf{covariance} of $X$ and $Y$ is then:

    \[Cov(X, Y) = \E{(X-\mu_X)(Y-\mu_Y)}\]

    The correlation is then:

    \[\rho = \rho_{X,Y} = \frac{Cov(X, Y)}{\sigma_X \sigma_Y}\]
\end{definition}

\begin{theorem}
    $Cov(X, Y) = \E{XY} - \E{X}\E{Y}$ and $\rho_{X,Y}$ satisfies $-1 \leq
    \rho_{X,Y} \leq 1$. If $Y = aX + b$, where $a,b$ are constants, then
    $\rho_{X,Y} = \begin{cases}-1,&a<0\\1,&a>0\end{cases}$. If $X, Y$ are
    independent, then $Cov(X, Y) = 0$, although the converse need not be true.
\end{theorem}

\begin{theorem}[Variance of a sum]
    $\Var{X + Y} = \Var{X} + \Var{Y} + 2Cov(X,Y)$. More generally $\Var{\sum_i
    a_i X_i} = \sum_i a_i^2 \Var{X_i} + \sum \sum_{i < j} 2*a_i*a_j Cov(X_i,
    X_j)$
\end{theorem}

\subsection{Expectation and Variance of Important Random Variables}

\subsection{Conditional Expectation}

\begin{definition}[Conditional Expectation]
\end{definition}

\begin{definition}[The Law of Iterated Expectation]
    $\E{\E{X | Y}} = \E{X}$
\end{definition}

\begin{definition}
\end{definition}

\begin{example}
    Suppose we pick a county from the US at random and choose $n$ people from
    it. Let $X$ be the number of these people with a disease. Let $Q$ be the
    proportion of people in the county with the disease. Then $X$ given $Q = q$
    is $Binomial(n, q)$. $\E{X | Q=q} = nQ$ and $\Var{X|Q=q} = nQ(1-Q)$.

    Suppose now $Q \sim Uniform(0, 1)$. This is a \textbf{hierarchical model}.
    $\E{X} = \E{\E{X}} = \E{nQ} = \frac{n}{2}$. $\Var{X} = \Var{\E{X|Q}} +
    \E{\Var{X|Q}}$. 

    \[\Var{\E{X|Q}} = \Var{nQ} = n^2\E{Q} = n^2\frac{1}{12}\]
    \[\E{\Var{X|Q}} = \E{nQ(1-Q)} = n\E{Q-Q^2} = \int{q - q^2 dq} = \frac{n}{6}\]

    Then:

    \[\Var{X} = \frac{n}{6} + \frac{n^2}{12}\]
\end{example}


\subsection{Moment Generating Functions}

\begin{definition}[The Moment Generating Function]
    The MGF, or Lapace Transformation, of $X$, is $\psi_{X}{t} = \E{e^{tX}} =
    \int e^{tX} dF_X dx$ where $t$ varies over $\R$.
\end{definition}

We will use the MGF to compute the moments of $X$. Assuming $\psi$ is well
defined on the open interval around $t=0$, $\psi'_X(0) = \frac{d}{dt}\,\E{e^{tX}}|_{t=0} = \E{\frac{d}{dt}e^{tX}}|_{t=0} = \E{Xe^{tX}}|_{t=0} = \E{X}$.

Swapping differentiation with expectation when $\psi$ is well defined in this
interval is a fact we will assume for now but (hopefully) will return to later.

\begin{theorem}[Properties of the MGF]
\item{If $Y = aX + b$, then $\psi_Y(t) = e^b\psi_X(at)$}
\item{}
\end{theorem}

\begin{theorem}
    Let $X$ and $Y$ be random variables. If $\psi_X(t) = \psi_Y(t)$ for all
    $t$, then $X$ and $Y$ are equal in distribution.
\end{theorem}

I view the above fact as another way of saying if $X$ and $Y$ have identical
moments, they must have the same distribution.

\subsection{Exercises}

\setcounter{exercise}{0}
\begin{exercise}
    Assume we have some fortune $c$ and we play a game where each turn we half
    or double our money with even probability. Compute the expected value of
    the resulting fortune after $n$ turns.
\end{exercise}

\begin{proof}
    Let $X_n$ be our random variable and see $\P(X_n = c*2^{n-2x}) =
    \binom{n}{x} 2^{-n} $. The density is binomial with support $c*2^{n-2x}$
    ranging over $x=0$ to $x=n$. The $2^{-n}$ expression comes from simplifying
    the standard $\frac{1}{2}^x \frac{1}{2}^{n-x}$. Similar for $2^{n-2x}$

    Then $\E{X_n} = \sum_{x=0}^n c*2^{n-2x} \binom{n}{x} 2^{-n}$. Simplifying
    the obvious things, we have $c \sum_{x=0}^n 2^{-2x} \binom{n}{x}$.
\end{proof}

\setcounter{exercise}{1}
\begin{exercise}
    Show $\Var{X} = 0$ iff exists some constant $c$ where $\P(X = c) = 1$.
\end{exercise}

\begin{proof}
    % TODO
\end{proof}

\setcounter{exercise}{2}
\begin{exercise}
    Let $X_1 \dots X_n$ be i.i.d. $\text{Uniform}(0, 1)$ and $Y_n = \max\{X_1 \dots
    X_n\}$. Compute $\E{Y_n}$.
\end{exercise}

\begin{proof}
    Observe $F_{Y_n} = \P(\max\{X_1 \dots X_n\} \leq y) = y^n$. (It is helpful to
    see also that $\P(\min\{X_1 \dots X_n\} \leq y) = 1-(1-y)^n$). Then
    $f_{Y_n} = dF_{Y_n} = \frac{d(y^n)}{dy} = ny^{n-1}$. $\E{Y_n} = \int y
    d_{Y_n} dy = \int  ny^n dy = [\frac{n}{n+1}y^{n+1}]_{y=0}^{1} =
    \frac{n}{n+1}$.
\end{proof}


\setcounter{exercise}{3}
\begin{exercise}
\end{exercise}

\begin{proof}
\end{proof}

\begin{note}
    Another approach is invoking the "rule of the lazy statistician", eg. $E{Y} = \int r(x) f_{X_1 \dots X_n} dx_1
    \dots dx_n$. In two dimensions, this calculation is trivial as the density
    can be evaluated as a piecewise integral over two halves of the unit square
    (those halves separated by a line through the diagonal). $2
    \int \int_{x_1 > x_2} x_1 dx_2 dx_1 = 2 * \frac{1}{3} = \frac{2}{3}$ as
    expected for $\E{Y_2}$
\end{note}

\setcounter{exercise}{4}
\begin{exercise}
    Flip a fair coin until you encounter a heads. Compute the expected value of
    the number of tosses.
\end{exercise}

\begin{proof}
    Let $X$ be the random variable holding the number of tosses. See that $\P(X
    = x) = \frac{1}{2}^x$. Then $\E{X} = \sum_{x=1}^{\infty} \frac{x}{2^x}$. To
    compute this series, recognize a general form of a geometric series can be
    expressed as $G(r) = \sum_{x=1}^{\infty} r^x = \frac{1}{1-r}$. 

    We need to rearrange things a bit:
    First, we take the derivative to pull out an x: $\frac{d(G(r))}{dr} = \sum_{x=1}^{\infty} xr^{x-1} =
    (1-r)^{-2}$. Then we multiply by r: $\sum_{x=1}^{\infty} xr^x =
    r*(1-r)^{-2}$.
    When $r=\frac{1}{2}$, this expression is equivalent to the series
    of our expectation. $\E{X} = 2$
\end{proof}

\setcounter{exercise}{6}
\begin{exercise}
    Let $X$ be a continuous random variable where $\P(X < 0) = 0$ and the
    expectation exists. Show $\E{X} = \int_{x=0}^{\infty} \P(X \geq x) dx$.
\end{exercise}

\begin{proof}
    Observe $\int_{x=0}^{\infty} \P(X \geq x) = \int_{x=0}^{\infty} (1 - F(x))
    dx$. This evaluates to $[(1-F(x))x]_{x=0}^{\infty} - \int_{x=0}^{\infty}
    (-f_X(x))x dx$ using integration by parts. Observing that $\lim_{x \to
        \infty} x\bigl( 1 - F_X(x) \bigr) = 0$, this expression simplifies to
        $\int_{x=0}^\infty x f_X(x) dx = \E{X}$ as desired.
\end{proof}

\begin{note}
    This is the tail-sum expression of expectation that will come in handy
    later.
\end{note}


\setcounter{exercise}{11}
\begin{exercise}
    Compute $\E{X}$ and $\Var{X}$ when $X$ is Poisson, Exponential.
\end{exercise}

\begin{proof}
    Let $X \sim Poisson(\lambda)$. Recall $e^{x} = \sum_{n=0}^{\infty}
    \frac{x^n}{n!}$ using the Maclaurin series for $e^{x}$. We are interested
    in $\sum_{x=0}^{\infty} x f_X(x) = \sum_{x=0}^{\infty} x
    \frac{\lambda^x}{x!} e^{-\lambda}$ for $\E{X}$. Notice the Maclaurin series for
    $xe^{x}$ is $\frac{nx^n}{n!}$. Then $ e^{-\lambda} \sum_{x=0}^{\infty}
    \frac{x\lambda^x}{x!} = e^{-\lambda}\lambda e^{\lambda} = \lambda$ as
    desired.

    To compute $\Var{X}$, we are instructed to first find $\E{X(X-1)}$. Notice
    $\E{X^2} = \E{X(X-1)} + \E{X}$.

    \[\E{X(X-1)} = \sum_{x=0}^{\infty} \frac{x(x-1)\lambda^x}{x!} e^{-\lambda}\]

    Simplify and notice this looks like a "shifted" form of the Maclaurin
    series for $e^{\lambda}$. The first two terms are 0, so we can start our
    counter at $x=2$:

    \[\sum_{x=2}^{\infty} \frac{\lambda^x}{(x-2)!} e^{-\lambda}\]

    Factoring out a $\lambda^2$ we arrive at the familiar series:

    \[e^{-\lambda} \lambda^2 \sum_{x=2}^{\infty} \frac{\lambda^{(x-2)}}{(x-2)!}
= e^{-\lambda}\lambda^2 e^\lambda = \lambda^2\]

    Indeed, $\E{X^2} = \E{X(X-1)} + \E{X} = \lambda^2 + \lambda$. So $\Var{X} =
    \E{X^2} - \E{X}^2 = \lambda^2 + \lambda - \lambda^2 = \lambda$ a desired.

    Now let $X \sim Exp(\beta)$. Recall $f_X(x) =
    \frac{1}{\beta}e^{-\frac{x}{\beta}}$. Then $\E{X} = \int_{x > 0}
    \frac{x}{\beta}e^{-\frac{x}{\beta}} dx$ Using integration by parts,
    $\frac{1}{\beta} \int xe^{\frac{-x}{\beta}} = \frac{1}{\beta} [\left. -\beta x
        e^{\frac{-x}{\beta}} - \beta^2 e^{\frac{x}{\beta}} \right|_{x=0}^{\infty} ] =
    \frac{1}{\beta} * \beta^2 = \beta$ as desired.

    Now we approach $\Var{X} = \E{X^2} - \E{X}^2$. First we compute $\E{X^2} =
    \int_{x > 0} \frac{x^2}{\beta}e^{-\frac{x}{\beta}} dx = \frac{1}{\beta}\int_{x > 0} x^2e^{-\frac{x}{\beta}}$.
    Again using integration by parts we arrive at: 
    \[\frac{1}{\beta}(\left. -\beta x^2 e^{\frac{-x}{\beta}} - \int (-\beta) 2x
        e^{\frac{-x}{\beta}})\right|_{x=0}^{\infty} = \]
    \[\frac{1}{\beta}(\left. -\beta x^2 e^{\frac{-x}{\beta}} +2\beta(-\beta x e^{\frac{-x}{\beta}} - \beta^2 e^{\frac{-x}{\beta}})) \right|_{x=0}^{\infty} = \]
    \[\frac{1}{\beta}(\left. -\beta x^2 e^{\frac{-x}{\beta}} - 2\beta^2 x
        e^{\frac{-x}{\beta}} - 2\beta^3 e^{\frac{-x}{\beta}})) \right|_{x=0}^{\infty} = \]
        \[\frac{1}{\beta}(2\beta^3) = 2\beta^2\]
        Then $\Var{X} = \E{X^2} - \E{X}^2 = 2\beta^2 - \beta^2 = \beta^2$ as
        desired.
\end{proof}


\setcounter{exercise}{12}
\begin{exercise}
    Suppose we generate a random variable in the following way. We flip a fair
    coin. If heads, $X \sim Uniform(0, 1)$, and if tails, $X \sim Uniform(3,
    4)$. Find the mean and standard deviation of $X$.
\end{exercise}

\begin{proof}
    Let $Y \sim Bernoulli(0.5)$ represent the coin flip.
    $\E{X} = \E{\E{X|Y}} = 0.5 * \E{X|Y=0} + 0.5 * \E{X|Y=1} = 0.5 * 0.5 + 0.5 * 3.5$

    $\Var{X} = \E{\Var{X|Y}} + \Var{\E{X|Y}}$. $\E{\Var{X|Y}} = 0.5 *
    \frac{1}{12} + 0.5 * \frac{1}{12} = \frac{1}{12}$. $\Var{\E{X|Y}} =
    \E{(\E{X|Y} - \E{X})^2} = 0.5 * (0.5 - 2)^2 + 0.5 * (3.5 -2)^2 = 1.5^2$.
    $\Var{X} = \E{\Var{X|Y}} + \Var{\E{X|Y}} = \frac{1}{12} + 2.25 = 2.33$
    $\sigma_X = 1.53$
\end{proof}

\setcounter{exercise}{20}
\begin{exercise}
    Let $X,Y$ be random variables. Suppose $\E{Y|X} = X$. Show $Cov(X, Y) =
    Var(X)$.
\end{exercise}

\begin{proof}
$Cov(X, Y) = \E{XY} - \E{X}\E{Y}$. Recognize $\E{XY} = \E{\E{XY|X}}$.
Evaluating the inner expectation $\E{XY|X} = X\E{Y|X} = X^2$. Then, $\E{XY}=
\E{X^2}$. Similarly, $\E{X}\E{Y} = \E{X}\E{\E{Y|X}} = \E{X}\E{X}$. So the
expression collapses to $\E{X^2} - \E{X}^2 = \Var{X}$.
\end{proof}

\setcounter{exercise}{22}
\begin{exercise}
    Find the MGFs for Poisson, Normal and Gamma distributions.
\end{exercise}

\begin{exercise}
    Let $X \sim Poisson(\lambda)$. Then $\psi_X(t) = \E{e^{Xt}} =
    \sum_{x=0}^{\infty} e^{Xt} e^{-\lambda} \frac{\lambda^x}{x!}$. Recognize
    $\sum_{x=0}^{\infty} \frac{e^{xt}\lambda^{x}}{x!} = \sum_{x=0}^{\infty}
    \frac{(e^t\lambda)^x}{x!}$ is the Maclaurin series for $e^{\lambda e^t}$.
    Then $e^{-\lambda} \sum_{x=0}^{\infty} e^{Xt} \frac{\lambda^x}{x!} =
    e^{-\lambda}e^{e^t\lambda} = e^{\lambda(e^t-1)}$.

    Let $X \sim Normal(\mu, \sigma^2)$.
    % TODO, finish completing the square

    % TODO Gamma
\end{exercise}

\setcounter{exercise}{5}
\begin{exercise}
    Prove the rule of the lazy statistician in the discrete case.
\end{exercise}
\begin{proof}
    Let $Y = r(X)$. $\E{Y} = \sum_{y \in Y} y \P(Y = y)$. For any given $y$, $y * \P(Y
    = y) = \sum_{x \in r^{-1}(y)} r(x) \P(X = x) = r(x) \sum_{x \in r^{-1}(y)}
    \P(X = x)$. Then $\sum_{y \in Y} y \P(Y = y) = \sum_{y \in Y} \sum_{x \in
    r^{-1}(y)} r(x) \P(X = x) = \sum_{x \in X} r(x) \P(X = x)$.
\end{proof}

\setcounter{exercise}{14}
\begin{exercise}
    Let
    \[
        f_{X,Y} = \begin{cases}
            \frac{1}{3}(x+y), &0\leq x \leq 1, 0 \leq y \leq 2 \\
            0, & o.w.
        \end{cases}
    \] and find $\Var{2X + 3Y + 8}$.
\end{exercise}

\begin{proof}
    The main idea is $\Var{2X + 3Y + 8} = 4\Var{X} + 9\Var{Y} + 2 * 2 * 3 Cov(X, Y)$. The rest is tedious algebra.

    The following calculations are important: $\E{X} = \frac{5}{9}$
    $\E{X^2} = \frac{7}{18}$
    $\E{Y} = \frac{11}{9}$
    $\E{Y^2} = \frac{16}{9}$
    $\Var{X} = \frac{13}{162}$
    $\Var{Y} = \frac{23}{81}$
    $\E{XY} = \frac{2}{3}$
    $Cov(X, Y) = -\frac{1}{81}$

    Plugging things back in, we arrive at $\frac{245}{81}$.
\end{proof}

\setcounter{exercise}{15}
\begin{exercise}
    Let $r(x)$ and $s(y)$ be functions of $x$ and $y$. Show that
    $\E{r(X)s(Y)|X} = r(X)\E{s(Y)|X}$. Then show $\E{r(X)|X} = r(X)$.
\end{exercise}

\begin{proof}
    $\E{r(X)s(Y)|X} = \int r(x')s(y) f_{X,Y|X}(x',y|x) dx' dy$. Pay careful
    attention to the use of $x'$ and $x$. We integrate over all $x'$ in $X$ where as $x$
    is provided by the conditioning $X$ (and may be fixed in future
    calculations).

    $\int r(x')s(y) f_{X,Y|X}(x',y|x) dx' dy = \int r(x')s(y) f_{Y|X}(y|x')
    f_{X|X}(x'|x) dx' dy$ by the chain rule of conditional densities. Now
    observe when $x$ is fixed, eg. in the expression $\E{r(X)s(Y)|X=x}$,
    $f_{X|X}(x'|x)$ becomes $1_{X=x}$ and $X$ degenerates to $X=x$. Indeed, 
    $\E{r(X)s(Y)|X=x} = r(x) \int s(y) f_{Y|X}(y|x') dx' dy = r(x) \E{s(Y)|X}$.
    When $X$ is not fixed, $\E{r(X)s(Y)|X} = r(X)\E{s(Y)|X}$
    % Tighten up issues.
    % https://chatgpt.com/c/68d572d6-d014-8328-8c82-5b8d5dfd0695
\end{proof}

\setcounter{exercise}{18}
\begin{exercise}
\end{exercise}

\begin{proof}
    $\E{\bar{X_n}} = \frac{1}{n}*n*\E{X_i} = \E{X_i}$
    $\Var{\bar{X_n}} = \frac{1}{n^2}*n*\Var{X_i} = \frac{1}{n} \Var{X_i}$
\end{proof}


\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\linewidth]{code/3.19.1.pdf}
  \caption{Expectation and variance of statistic as function of n}
  \label{fig:lognormal}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\linewidth]{code/3.19.2.pdf}
  \caption{Sampling distribution for increasing n}
  \label{fig:lognormal}
\end{figure}

\setcounter{exercise}{21}
\begin{exercise}
    Let $0 < a < b < 1$ and $X \sim Uniform(0, 1)$. Let
    \[
        Y =
        \begin{cases}
        1, & 0 < X < b, \\
        0, & \text{otherwise},
        \end{cases}
        \qquad
        Z =
        \begin{cases}
        1, & a < X < 1, \\
        0, & \text{otherwise}.
        \end{cases}
    \]
    \begin{itemize}
        \item{Show $Y$ and $Z$ are not independent.}
        \item{Evaluate $\E{Y | Z}$}
    \end{itemize}
\end{exercise}

\begin{proof}
    \begin{itemize}
        \item{$\P(Y=1,Z=1) = b-a \neq \P(Y=1)\P(Z=1) = a*(1-a)$}
        \item{
        $\E{Y | Z} = $
        $\begin{cases}
        \frac{b-a}{2-a}, & Z=1, \\
        1, & Z=0.
        \end{cases}$}
    \end{itemize}
\end{proof}

\section{Inequalities}

\subsection{Probability Inequalities}

\begin{definition}[Markov's Inequality]
    For some nonnegative r.v. $X$ and $t > 0$:
    \[ \P(X \geq t) \leq \frac{\E{X}}{t} \]
\end{definition}

\begin{definition}[Chebyshev's Inequality]
    Let $\mu = \E{X}$ and $\sigma^2 = \Var{X}$, then
    \[ \P(|X-\mu| \geq t) \leq \frac{\sigma^2}{t^2}\] 
    and \[\P(|Z| \geq k) \leq \frac{1}{k^2}\] 
    where $Z = \frac{X - \mu}{\sigma}$. In particular, $\P(Z \geq 2) \leq \frac{1}{4}$ and $\P(Z \geq 3) \leq \frac{1}{9}$
\end{definition}

\begin{proof}
    $\frac{\E{(X - \mu)^2}}{t^2} \geq \P((X-\mu)^2 \geq t^2) = \P(|X-\mu| \geq
    t)$. The second case comes from $t = k\sigma$: $\P(|X-\mu| \geq k\sigma)
    \leq \frac{\sigma^2}{k^2\sigma^2}$. Then $\P(|Z| \geq k) \leq \frac{1}{k^2}$
\end{proof}

\begin{definition}[Hoeffding's Inequality]
    Let $Y_1 \dots Y_n$ be independent observations s.t. $\E{Y_i} = 0$ and $a_i
    \leq Y_i \leq b_i$. Let $\epsilon > 0$. For any $t > 0$, $\P((\sum_{i=0}^{n} Y_i)
    \geq \epsilon) \leq e^{-t\epsilon}\prod_{i=0}^n e^{t^2\frac{(a_i-b_i)}{8}}$
\end{definition}

\begin{definition}[Bernoulli Presentation of Hoeffding's Inequality]
    Let $X_1 \dots X_n \sim Bernoulli(p)$ with $\epsilon > 0$. Then:
    \[ \P(|\bar{X_n} - p| \geq \epsilon) \leq 2e^{-2n\epsilon^2}\]
\end{definition}

\begin{proof}
    Using Markov's bound, $\P(\sum_{i=0}^{n} Y_i \geq t\epsilon) =
    \P(e^{\sum_{i=0}^{n} Y_i} \geq e^{t\epsilon}) \leq
    \frac{\E{e^{\sum_{i=0}^{n} Y_i}}}{e^{t\epsilon}} =
    e^{-te}\prod_{i=0}^{n}e^{tY_i}$.

    Now consider $\E{e^{tY_i}}$. We know $a_i \leq Y_i \leq b_i$, so we can
    express $Y_i$ as a convex combination $Y_i = (1-\alpha)a_i + \alpha b_i$
    where $\alpha = \frac{Y_i - a_i}{b_i - a_i}$. Because $e^x$ is a convex
    function, $e^{tY_i} \leq (1-\alpha)e^{t a_i} + \alpha e^{t b_i}$. Because
    $\E{Y_i} = 0$, $\E{\alpha} = \frac{-a_i}{b_i - a_i}$. Indeed, $\E{e^{tY_i}}
    \leq \E{(1-\alpha)e^{t a_i} + \alpha e^{t b_i}} = \frac{b_ie^{a_i} - a_ie^{b_i}}{b_i - a_i} = e^{g(u)}$ for some $u, g$.

    We will make use of the exact form of Taylor's theorem: if g is a smooth
    function then there is a number $\xi \in (0, u)$ such that $g(u) = g(0) +
    ug'(0) + \frac{u^2}{2}g''(\xi)$. 

    Some algebra is needed to $\frac{b_ie^{a_i} - a_ie^{b_i}}{b_i - a_i} = e^{g(u)}$
\end{proof}

\begin{note}
    Hoeffding's inequality gives us a convenient way of constructing confidence
    intervals. %TODO
\end{note}

\begin{definition}[Mill's Inequality]
    Let $Z \sim Normal(0, 1)$. For $t > 0$, $\P(|Z| \geq t) \leq
    \sqrt{\frac{2}{\pi}} \frac{e^{\frac{-t^2}{2}}}{t}$
\end{definition}

This result comes from another manipulation of the Markov's bound. We work this
out in detail in the exercises.

\begin{exercise}
Let $X \sim Exponential(\beta)$. Find $\P(|X - \mu_X| < k\sigma_X)$ for $k >
1$. Compare this to the bound you get from Chebyshev's inequality.
\end{exercise}

\begin{proof}
    Notice $\P(|X - \mu_X| > k\sigma_X) = \P(|X - \beta| > k\beta) = 1 - \P(|X - \beta| < k\beta)$. Write the
    central event $|X - \beta| < k\beta \iff \beta - k\beta < X < \beta +
    k\beta$. If $k > 1$, then $\beta - k\beta$ lies outside our support so our
    event simplifies to $\P(X < \beta + k\beta) = \P(X < \beta(1 + k)) =
    \int_{x=0}^{\beta (1+k)} f_X(x) dx = -e^{1+k} + 1$. $\P(|X - \mu_X| >
    k\sigma_X) = 1 - (-e^{1+k} + 1) = e^{1+k}$.

    Chebyshev's bound is simply $\frac{1}{k^2}$.
\end{proof}

\begin{note}
Notice this bound goes like $e^{k}$, while the Chebyshev bound goes like
$\frac{1}{k^2}$. For distributions with long exponential tails, this bound
rapidly becomes quite poor.
\end{note}

\begin{exercise}
    Let $X \sim Poisson(\lambda)$. Use Chebyshev's to show $\P(X \geq 2\lambda)
    \leq \frac{1}{\lambda}$
\end{exercise}

\begin{proof}
    $\P(|X - \lambda| \geq \lambda) \leq \frac{\lambda}{\lambda^2} = \P(X \geq
    2\lambda) \leq \frac{1}{\lambda}$. (Notice $\{ |X - \lambda | \geq \lambda \}
    \supset \{ X \geq 2\lambda \}$)
\end{proof}

\begin{exercise}
    Let $X_1 \dots X_n \sim Bernoulli(p)$ and $\bar{X_n} =
    n^{-1}\sum_{i=0}^{n}X_i$. Bound $\P(|\bar{X_n} - p| \geq \epsilon)$ using
    Chebyshev's and Hoeffding's inequality. Show that Hoeffding's inequality
    produces a tighter bound when $n$ is large.
\end{exercise}

\begin{proof}
    By Chebyshev's (recall for i.i.d. $X_i$, $\E{n^{-1}\sum_{n=0}^{i} X_i} =
    \E{X_i}$), $\P(|X_n - p| \geq \epsilon) \leq \frac{p(1-p)}{n\epsilon^2} \leq
    \frac{1}{n4\epsilon^2}$, where the second bound comes from the fact that
    $\frac{1}{4}$ is the largest value of $\Var{X}$ over $p$. We use the
    Bernoulli presentation of Hoeffding inequality and arrive at $2e^{2*n\epsilon^2}$

    Let $\epsilon=0.2$ and examine the bounds for $n=10, 100, 1000$. The
    Chebyshev bound goes like $0.625, 0.0625, 0.00625$ while the Hoeffding goes
    like $0.8, 0.00067, 3.6e-35$. Notice Chebyshev starts out stronger and
    quickly becomes order(s) of magnitude weaker.
\end{proof}

\begin{exercise}
    Let $X_1 \dots X_n \sim Bernoulli(p)$. Fix $\alpha > 0$. Define
    $\epsilon_n = \sqrt{\frac{1}{2n}log(\frac{2}{\alpha})}$, $\hat{p} = n^{-1}
    \sum_{i=0}^n X_i$, and $C_n = (\hat{p}-\epsilon, \hat{p}+\epsilon)$.
    \begin{itemize}
        \item{Use Hoeffding's to show $\P(C_n \text{ contains } p) \geq 1 -
            \alpha$}
        \item{Fix $\alpha=0.05$ and $p=0.4$. Conduct a simulation study with a
            computer to see how often the interval contains p (called coverage) for different
        values of $n$ between 1 and 10000. Plot the coverage as a function of
    n.}
        \item{Plot the length of the interval versus n. Suppose we want the
            interval to be less than 0.05. How large should n be?}
    \end{itemize}
\end{exercise}

\begin{proof}
    \begin{itemize}
        \item{Using the Bernoulli presentation of Hoeffding's given,
            $\P(|\hat{X_n} - p| \geq \epsilon) \leq 2e^{-2n\epsilon^2}$.
        Equivalently, $\P(|\hat{X_n} - p| \leq \epsilon) \geq 1 -
        2e^{-2n\epsilon^2}$. Notice $\hat{p_n} = \bar{X_n}$ from this original
        definition. Plugging in $\epsilon_n$, we have $\P(|\hat{p_n} - p| \leq
        \epsilon_n) \geq 1 - \alpha$} which is the same as saying $\P(C_n
        \text{ contains } p) \geq 1 - \alpha$ as desired.$  \item{%TODO}
    \item{The interval shrinks roughly like $\sqrt{n}$.
        $\sqrt{\frac{log(\frac{2}{\alpha})}{2n}} \leq 0.025$ then $n \geq 2960$}
    \end{itemize}
\end{proof}

\begin{exercise}
    Prove Mill's inequality: $\P(|Z| > t) \leq
    \sqrt{\frac{2}{\pi}}\frac{e^{-\frac{t^2}{2}}}{t}$
\end{exercise}

\begin{proof}
    Observe $\P(|Z| > t) = 2\P(Z > t)$. We'll begin with one side. Let $\phi(x)
    = \frac{1}{\sqrt{2\pi}} e^{\frac{x^2}{2}}$ be the PDF of $Z$. $\P(Z > t) =
    \int_{x=t}^{\infty} \phi(x) dx$. Because $\frac{x}{t} > 1$ when $x > t$,
    $\int_{x=t}^{\infty} \phi(x) dx \leq \int_{x=t}^{\infty} \frac{x}{t}
    \phi(x) dx$. Notice $\phi'(x) = -x\phi(x)$. Then $\int_{x=t}^{\infty}
    \frac{x}{t} \phi(x) dx = \frac{1}{t}[-\phi(x)]_{x=t}^{\infty} =
    \frac{\phi(t)}{t}$.

    Then $\P(|Z| > t) = 2\P(Z > t) \leq 2 \frac{\phi(t)}{t} =
    \sqrt{\frac{2}{\pi}} \frac{1}{t} e^{\frac{-t^2}{2}}$ as desired.
\end{proof}

\subsection{Inequalities for Expectation}

% https://chatgpt.com/c/689b6b5f-4d00-832e-b574-8f6572915d59

\section{Convergence}

\subsection{Types of Convergence}

\begin{proof}

Now we prove (b). Fix $\epsilon > 0$ and let $x$ be a continuity point of $F$. $F_n(x) =
\P(X_n \leq x) = \P(X_n \leq x, X \leq x + \epsilon) + \P(X_n \leq x, X \geq x
+ \epsilon) \leq \P(X \leq x + \epsilon) + \P(|X_n - X| \geq \epsilon) \leq
F(x+\epsilon) + \P(|X - X_n| \geq \epsilon)$

Similarly, $F(x-\epsilon) = \P(X \leq x - \epsilon, X_n \leq x) + \P(X \leq x -
\epsilon, X_n \geq x) \leq F_n(x) + \P(|X-X_n| \geq \epsilon)$.

We use these bounds to construct the inequality $F(x-\epsilon) - \P(|X-X_n|\geq
\epsilon)\leq F_n(x) \leq F(x) + \P(|X-X_n|\geq \epsilon)$. Take the limit as
$n$ goes to $\infty$. Notice that $F_n(x)$ is a sequence with no guarantees of
convergence, so we have to account for the largest and smallest elements. By our assumption $\P(|X-X_n|\geq \epsilon) \xrightarrow{} 0$ as $n \xrightarrow{} \infty$.

$F(x-\epsilon) \leq \lim \inf F_n(x) \leq \lim \sup F_n(x) \leq F(x+\epsilon)$

Now take the limit as $\epsilon \xrightarrow 0$ (this statement holds for
arbitrary $\epsilon > 0$). $F(x^-) \leq \lim \inf F_n(x) \leq \lim \sup F_n(x)
\leq F(x^+)$. Using continuinty of $F$ at $x$, $F(x^-) = F(x^+)$ and $F_n(x) =
F(x)$ as desired.
\end{proof}

\begin{note}[Convergence in probability does not imply convergence in quadratic mean]
    Let $U \sim Uniform(0, 1)$ and $X_n = \sqrt{n}I_{(0, \frac{1}{n})}(U)$.
    First see $X_n \xrightarrow{P} 0$. Indeed as $n$ grows sufficiently large, for
    arbitrary $\epsilon$, $\P(|X_n| \geq \epsilon) = \P(0 \geq U < n) = n
    \xrightarrow{} 0$. However, $\E{X_n^2} = \int_{u=0}^{\frac{1}{n}} n du =
    1$.
\end{note}

\begin{note}[Convergence in distribution does not imply convergence in probability]
    Let $X \sim Normal(0, 1)$. For each $n$, let $X_n = -X$. $F_n(x) = F(x)$
    for any $x$. But $\P(|X - X_n| \geq \epsilon) = \P(|2X| \geq \epsilon) = \P(|X|
    \geq \frac{\epsilon}{2}) \neq 0$.  Indeed the symmetrical shape of the
    Gaussian preserves the CDF but any given values are negatives of each other
    and will never converge.
\end{note}

\subsection{The Law of Large Numbers}

A crowned jewel of probability. The sample mean approaches the expectation of
the underlying distribution.

\begin{definition}
    Let $X_1 \dots X_n$ be i.i.d. with $\E{X_i} = \mu$ and $\Var{X_i} =
    \sigma^2$. Let $\bar{X_n} = n^{-1}\sum_{i=0}^n X_i$. Then $\bar{X_n}
    \xrightarrow{P} \mu$.
\end{definition}

The sample mean is a \textbf{random variable} so it will never numerically
equal the expectation. It will cluster closer and closer to it.

\subsection{The Central Limit Theorem}

While the LLN tells us the sample mean clusters around the true mean, it does
not give us tools to approximate statements about probability.

\begin{definition}[CLT]
    Let $X_1 \dots X_n$ be i.i.d. with $\E{X_i} = \mu$ and $\Var{X_i} =
    \sigma^2$. Let $\bar{X_n} = n^{-1}\sum_{i=0}^n X_i$. Then $Z_n =
    \frac{\sqrt{n}(\bar{X_n}-\mu)}{\sigma} \xrightarrow{D} N(0, 1)$

    In other words, $\lim_{n \xrightarrow{} \infty} \P(Z_n \leq z) = \P(Z \leq z) = \phi(z)$
\end{definition}

Note we use this to approximate \textbf{probability statements} not the
distribution $\bar{X_n}$ itself. 

These approximations are not perfect and indeed we can bound the error:

\begin{definition}[The Berry-Esseen Inequality]
    $sup_x |\P(Z_n \leq x) - \phi(x)| \leq \frac{37}{4}\frac{\E{|X_i-\mu|^3}}{\sqrt{n}\sigma^3}$
\end{definition}

Where the $sup$ bounds the difference across all possible $x$ in the domain.

\subsection{Delta Method}

This is like the CLT for functions of the sample mean.

\end{document}
