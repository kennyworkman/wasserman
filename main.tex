\documentclass[10pt]{article}
\usepackage{kennyworkman}

\title{Wasserman: All of Statistics}
\author{Kenny Workman}
\date{\today}

\begin{document}

\maketitle

\section{Probability}

\subsection{Basics}

\begin{definition}
    The \textbf{sample space} $\Omega$ is the set of outcomes from an experiment. Each
    point is denoted $\omega$ and subsets, eg. $A \subset \Omega$ are called
    \textbf{events}.
\end{definition}

\begin{definition}[Axioms of Probability]
    A function $\P: \Omega \to \R$ that assigns a real number to each event $A
    \subset \Omega$ is called a \textbf{probability function} or
    \textbf{probability measure} if it satsifies these three axioms:
    \begin{enumerate}
        \item \textbf{Non-negativity}. $\P(A) \ge 0$ for every event $A$
        \item \textbf{Normalization}. $\P(\Omega) = 1$.
        \item \textbf{Additivity}. $\P(A\cup B) = \P(A) + \P(B)$ if $A \cap B = \emptyset$.
    \end{enumerate}
\end{definition}

It is incredible, and not obvious, that much of probability is built up from
these only these three axioms

\begin{example}

It's actually tricky to show $\P(A \cup B) = \P(A) + \P(B) - \P(A \cap B)$ with
these three facts:

\begin{align*}
\P(A \cup B) &= \P(AB^c \cap AB \cap A^cB) \\
&= \P(AB^c) + \P(AB) + \P(A^cB) \\
&= \P(AB^c) + \P(AB) + \P(A^cB) + P(AB) - P(AB) \\
&= \P(AB^c \cup AB) + \P(A^cB \cup AB) - P(AB) \\
&= \P(A) + \P(B) - P(AB)
\end{align*}
\end{example}

Another simple idea is that events that are identical at the limit should have
identical probabilities.

\begin{theorem}[Continuity of Events]
If $A_n \to A$ then $\P(A_n) \to \P(A)$.
\end{theorem}

\begin{proof}
    Let $A_n$ be monotone increasing: $A_1 \subset A_2 \subset \dots$. Let
    $A = \lim_{n \to \infty} A_n = \bigcup_{i=1}^{\infty} A_i$.

    Construct disjoint sets $B_i$ from each $A_i$ where $B_1 = A_1$ and $B_n =
    \{ \omega \in \Omega : \omega \in A_n,\omega \notin \bigcup_{i=1}^{i-1}A_i \}$. It will be
    shown that (1) each pair of $B_i$ are disjoint, (2) $\bigcup_{i=1}^n A_i =
    \bigcup_{i=1}^n B_i$ and (3) $A = \bigcup_{i=1}^\infty A_i =
    \bigcup_{i=1}^\infty B_i$ (Exercise 1.1).

    From Axiom 3: $\P(A_n) = \P(\bigcup\limits_{i=1}^n A_i) = \P(\bigcup\limits_{i=1}^n B_i) =
    \sum\limits_{i=1}^n \P(B_i)$.

    Then $\lim_{n \to \infty} \P(A_n)
    = \lim_{n \to \infty} \sum\limits^{n}\P(B_n)
    = \sum\limits^{\infty}\P(B_n)
    = \P(\bigcup\limits^{\infty}B_n)
    = \P(A)$

\end{proof}

\begin{definition}[Conditional Probability]
    If $\P(B) > 0$, then the probability of $A$ given $B$ is 
    \[\P(A \mid B) = \dfrac{\P(AB)}{\P(B)}.\]
\end{definition}

\begin{exercise}
    Fill in the details for Theorem 1.2 and extend to the case where $A_n$ is monotone decreasing.
\end{exercise}

\begin{proof}
    For any pair $B_{n+1}$ and $B_n$, because $B_n \subset A_n$ and $B_{n+1}
    \cap A_n = \emptyset$, it follows that $B_{n+1} \cap B_n = \emptyset$.

    Let $\bigcup_{i=1}^n B_i = \bigcup_{i=1}^n A_i$. Then $\bigcup_{i=1}^{n+1}
    B_i = (A_{n+1} \setminus \bigcup_{i=1}^n A_i) \bigcup (\bigcup_{i=1}^n
    A_i) = \bigcup_{i=1}^{n+1} A_i$.

    For the monotone decreasing case, let $A_n$ be a sequence where $A_1 \supset A_2 \supset A_3 \dots$. 

    Observe $A_1^c \subset A_2^c \dots$ and $\lim_{n \to \infty} A_n = \Omega
    \setminus \bigcup^{\infty} A_i^c$. Construct disjoint $B^c_n$ from $A^c$ in
    the same way.

    Then $\lim_{n \to \infty} \P(A_n) = 1 - \sum\limits^{\infty} \P(B^c_i) = 1 -
    \P(A^c) = \P(A)$
\end{proof}


\setcounter{exercise}{2}
\begin{exercise}
    Let $\Omega$ be a sample space and $A_1, A_2, \dots$ be events. Define
    $B_n = \cup_{i=n}^{\infty} A_i$ and $C_n = \cap_{i=n}^{\infty} A_i$.
    \begin{enumerate}[(a)]
        \item{Show $B_1 \supset B_2 \supset B_3 \dots$ and $C_1 \subset C_2
            \subset C_3 \dots$}
        \item{Show $\omega \in \cap_{n=1}^{\infty} B_n$ iff $\omega$ is in an
            infinite number of the events}
        \item{Show $\omega \in \cup{n=1}^{\infty} C_n$ iff $\omega$ belongs to all
                of the events, except possibly a finite number of those
            events.}
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[(a)]
        \item{Certainly $\cup_{i=1}^{\infty} A_i \supset \cup_{i=2}^{\infty} A_i \dots$ and $\cap_{i=1}^{\infty} A_i \subset \cap_{i=2}^{\infty} \dots$.}
        \item{Forward. Assume $\omega \in \cap_{n=1}^{\infty}B_n$. If $\omega$
            does not belong to an infinite number of events $A_i$, there exists
        some index $j$ past which $\omega \notin B_j$. Then certainly $\omega \notin \cap_{n=1}^{\infty}B_n$. Reverse. $\omega$ belonging to infinite events means there cannot exist such a $j$ described previously so $\omega \in B_n$ for all $n$. Indeed $\omega \in \cap_{n=1}^{\infty}B_n$}
    \item{Forward. Assume $\omega \in \cup_{n=1}^{\infty}C_n$. Then $\omega \in
        C_j = \cap_{i=j}^{\infty} A_i$ for some $j$. This is another way of
    saying $\omega$ is in every single event except for perhaps a finite number
in $A_{i < j}$. Reverse. Let $j$ be the index of the largest event that
$\omega$ is not in. Then $\omega \in C_{n > j}$ and certainly $\omega \in
\cup^{\infty}C_n$.}
    \end{enumerate}
\end{proof}

\begin{note}
    The key idea above is this notion of "infinitely often" (i.o.) and "all but finitely
    often" (eventually) which are two distinct structures of infinite occurence in
    sequences. Consider an $\omega$ that exists in every other event (eg. just the
    odd indices) for infinite events and revisit its inclusion in $\cap^{\infty}
    B_i$ and $\cup^{\infty} C_i$.
\end{note}

\begin{note}
    % https://chatgpt.com/c/67c5f3d7-48f4-8004-a28f-8f25f47b44c4
    $\lim \cap \cup A_n$ is also referred to as the limit infimum of $A_n$. Similarly,
    $\lim \cup \cap A_n$ is referred to as the limit supremum of $A_n$.
    %TODO: show equivalence
    % Then revisit Borel Cantelli lemma: https://en.wikipedia.org/wiki/Borel%E2%80%93Cantelli_lemma
\end{note}

\setcounter{exercise}{6}
\begin{exercise}
    Let $\P(\bigcup\limits^n A_i) \leq \sum\limits^n \P(A_i)$. Then
    $\P(A_{n+1} \cup (\bigcup\limits^n A_i)) \leq \P(A_{n+1}) +
    (\sum\limits^n \P(A_i)) - \P(A_{n+1} \cap(\bigcup\limits^n A_i)) \leq
    \sum\limits^{n+1}\P(A_i)$
\end{exercise}

\begin{note}
% https://en.wikipedia.org/wiki/Boole%27s_inequality
    Expand a bit on the Boole inequality.
\end{note}

\setcounter{exercise}{8}
\begin{exercise}
    For fixed $B$ s.t. $\P(B) > 0$, show $\P(.\mid B)$ satisfies the three
    axioms of probability.
\end{exercise}
\begin{proof}
    \begin{itemize}
        \item{\textbf{Non-negativity}. If $\P(B) > 0$ and $\P(AB) > 0$ for any
                $A \subset \Omega$, certainly
            $\frac{\P(AB)}{\P(B)} > 0$}.
        \item{\textbf{Normalization}. $\frac{\P(\Omega \cap B)}{\P(B)} =
            \frac{\P(B)}{\P(B)} = 1$}
        \item{\textbf{Additivity}. Let $AB \cap CB = \emptyset$, then $\P(AB
            \cap CB) = \P(AB) + \P(CB)$. Indeed $\frac{\P(AB\cap CB)}{B} =
        \frac{\P(AB)}{B} + \frac{\P(CB)}{B}$}
    \end{itemize}
\end{proof}

\setcounter{exercise}{10}
\begin{exercise}
    Suppose $A$ and $B$ are independent events. Show that $A^c$ and $B^c$ are
    also independent.
\end{exercise}
\begin{proof}
    We are given $\P(AB) = \P(A)\P(B)$. Then $\P(A^c)\P(B^c) = (1 - \P(A))(1
    - \P(B)) = 1 - \P(A) - \P(B) + \P(A)\P(B) = 1 - \P(A\cup B) = \P(A^cB^c)$.
    The second to last equality uses independence of $P(AB)$. The last equality
    uses the property of set complements $P(A \cup B) = P(A^c \cap B^c)$.
\end{proof}

\setcounter{exercise}{12}
\begin{exercise}
    Suppose a fair coin is tossed repeatedly until heads and tails is each
    encoutered exactly once. Describe $\Omega$ and compute the probablity
    exactly three tosses are needed.
\end{exercise}
\begin{proof}
    \begin{itemize}
        \item{The sample space is the set of binary strings with exactly one
            $0$ and $1$. For strings of length greater than 2, these are
        repeated strings of $0$ or $1$ capped with a $1$ or $0$ respectively.}
    \item{By independence, each n-string has an identical probability
        $\frac{1}{2}^n$. There are two such 3-strings: 001 and 110. Using additivity,
    $\P(\text{3 tosses}) = \frac{1}{8} + \frac{1}{8} = \frac{1}{4}$}
    \end{itemize}
\end{proof}

\setcounter{exercise}{14}
\begin{exercise}
    The probability a child has blue eyes is $\frac{1}{4}$. Assume independence
    between children. Consider a family with 3 children.
    \begin{itemize}
        \item{If it is known that at least one of the children have blue eyes,
            what is the probablity that at least two of the children have blue
        eyes?}
    \item{If it is know that the youngest child has blue eyes, what is the
        probability that at least two of the children have blue eyes?}
    \end{itemize}
\end{exercise}
\begin{proof}
    \begin{itemize}
        \item{Straightforward conditional probability. Let $A$ be the event
                where at least one child has blue eyes and $B$ be the event
                where at least two children have blue eyes. Consider first,
                $\P(A) = 1 - \P(\text{no child has blue eyes}) =
                1 - \frac{27}{64} = \frac{37}{64}$. Compute $\P(A\cap B)$ by
                enumerating events 101, 111, 110 and using additivity: $2\cdot
                \frac{1}{4}^2\cdot\frac{3}{4} + \frac{1}{4}^3 = \frac{10}{64}$. Then $\P(B\mid A) = \frac{\P(A
            \cap B)}{\P(A)} = \frac{10}{64}\cdot\frac{64}{37} = \frac{10}{37}$}
        \item{Similar procedure. Let $A$ be the event where the youngest child
            has blue eyes and $B$ be as before. Using independence, $\P(A) =
        \frac{1}{4}$. (To see this rigorously, enumerate the sample space and
    see $\P(\Omega\mid \text{first child blue}) = 1$). Now $\P(B \cap A)$
    describe events 110, 101, 111 only. $\frac{7}{64}\cdot\frac{4}{1} =
\frac{7}{16}$.}
    \end{itemize}
\end{proof}

\end{document}
