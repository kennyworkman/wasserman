\documentclass[10pt]{article}
\usepackage{kennyworkman}

\title{Wasserman: All of Statistics}
\author{Kenny Workman}
\date{\today}

\begin{document}

\maketitle

\section{Probability}

\subsection{Basics}

\begin{definition}
    The \textbf{sample space} $\Omega$ is the set of outcomes from an experiment. Each
    point is denoted $\omega$ and subsets, eg. $A \subset \Omega$ are called
    \textbf{events}.
\end{definition}

\begin{definition}[Axioms of Probability]
    A function $\P: \Omega \to \R$ that assigns a real number to each event $A
    \subset \Omega$ is called a \textbf{probability function} or
    \textbf{probability measure} if it satsifies these three axioms:
    \begin{enumerate}
        \item \textbf{Non-negativity}. $\P(A) \ge 0$ for every event $A$
        \item \textbf{Normalization}. $\P(\Omega) = 1$.
        \item \textbf{Additivity}. $\P(A\cup B) = \P(A) + \P(B)$ if $A \cap B = \emptyset$.
    \end{enumerate}
\end{definition}

It is incredible, and not obvious, that much of probability is built up from
these only these three axioms

\begin{example}

It's actually tricky to show $\P(A \cup B) = \P(A) + \P(B) - \P(A \cap B)$ with
these three facts:

\begin{align*}
\P(A \cup B) &= \P(AB^c \cap AB \cap A^cB) \\
&= \P(AB^c) + \P(AB) + \P(A^cB) \\
&= \P(AB^c) + \P(AB) + \P(A^cB) + P(AB) - P(AB) \\
&= \P(AB^c \cup AB) + \P(A^cB \cup AB) - P(AB) \\
&= \P(A) + \P(B) - P(AB)
\end{align*}
\end{example}

Another simple idea is that events that are identical at the limit should have
identical probabilities.

\begin{theorem}[Continuity of Events]
If $A_n \to A$ then $\P(A_n) \to \P(A)$.
\end{theorem}

\begin{proof}
    Let $A_n$ be monotone increasing: $A_1 \subset A_2 \subset \dots$. Let
    $A = \lim_{n \to \infty} A_n = \bigcup_{i=1}^{\infty} A_i$.

    Construct disjoint sets $B_i$ from each $A_i$ where $B_1 = A_1$ and $B_n =
    \{ \omega \in \Omega : \omega \in A_n,\omega \notin \bigcup_{i=1}^{i-1}A_i \}$. It will be
    shown that (1) each pair of $B_i$ are disjoint, (2) $\bigcup_{i=1}^n A_i =
    \bigcup_{i=1}^n B_i$ and (3) $A = \bigcup_{i=1}^\infty A_i =
    \bigcup_{i=1}^\infty B_i$ (Exercise 1.1).

    From Axiom 3: $\P(A_n) = \P(\bigcup\limits_{i=1}^n A_i) = \P(\bigcup\limits_{i=1}^n B_i) =
    \sum\limits_{i=1}^n \P(B_i)$.

    Then $\lim_{n \to \infty} \P(A_n)
    = \lim_{n \to \infty} \sum\limits^{n}\P(B_n)
    = \sum\limits^{\infty}\P(B_n)
    = \P(\bigcup\limits^{\infty}B_n)
    = \P(A)$

\end{proof}

\begin{definition}[Conditional Probability]
    If $\P(B) > 0$, then the probability of $A$ given $B$ is 
    \[\P(A \mid B) = \dfrac{\P(AB)}{\P(B)}.\]
\end{definition}

\begin{exercise}
    Fill in the details for Theorem 1.2 and extend to the case where $A_n$ is monotone decreasing.
\end{exercise}

\begin{proof}
    For any pair $B_{n+1}$ and $B_n$, because $B_n \subset A_n$ and $B_{n+1}
    \cap A_n = \emptyset$, it follows that $B_{n+1} \cap B_n = \emptyset$.

    Let $\bigcup_{i=1}^n B_i = \bigcup_{i=1}^n A_i$. Then $\bigcup_{i=1}^{n+1}
    B_i = (A_{n+1} \setminus \bigcup_{i=1}^n A_i) \bigcup (\bigcup_{i=1}^n
    A_i) = \bigcup_{i=1}^{n+1} A_i$.

    For the monotone decreasing case, let $A_n$ be a sequence where $A_1 \supset A_2 \supset A_3 \dots$. 

    Observe $A_1^c \subset A_2^c \dots$ and $\lim_{n \to \infty} A_n = \Omega
    \setminus \bigcup^{\infty} A_i^c$. Construct disjoint $B^c_n$ from $A^c$ in
    the same way.

    Then $\lim_{n \to \infty} \P(A_n) = 1 - \sum\limits^{\infty} \P(B^c_i) = 1 -
    \P(A^c) = \P(A)$
\end{proof}


\setcounter{exercise}{2}
\begin{exercise}
    Let $\Omega$ be a sample space and $A_1, A_2, \dots$ be events. Define
    $B_n = \cup_{i=n}^{\infty} A_i$ and $C_n = \cap_{i=n}^{\infty} A_i$.
    \begin{enumerate}[(a)]
        \item{Show $B_1 \supset B_2 \supset B_3 \dots$ and $C_1 \subset C_2
            \subset C_3 \dots$}
        \item{Show $\omega \in \cap_{n=1}^{\infty} B_n$ iff $\omega$ is in an
            infinite number of the events}
        \item{Show $\omega \in \cup{n=1}^{\infty} C_n$ iff $\omega$ belongs to all
                of the events, except possibly a finite number of those
            events.}
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[(a)]
        \item{Certainly $\cup_{i=1}^{\infty} A_i \supset \cup_{i=2}^{\infty} A_i \dots$ and $\cap_{i=1}^{\infty} A_i \subset \cap_{i=2}^{\infty} \dots$.}
        \item{Forward. Assume $\omega \in \cap_{n=1}^{\infty}B_n$. If $\omega$
            does not belong to an infinite number of events $A_i$, there exists
        some index $j$ past which $\omega \notin B_j$. Then certainly $\omega \notin \cap_{n=1}^{\infty}B_n$. Reverse. $\omega$ belonging to infinite events means there cannot exist such a $j$ described previously so $\omega \in B_n$ for all $n$. Indeed $\omega \in \cap_{n=1}^{\infty}B_n$}
    \item{Forward. Assume $\omega \in \cup_{n=1}^{\infty}C_n$. Then $\omega \in
        C_j = \cap_{i=j}^{\infty} A_i$ for some $j$. This is another way of
    saying $\omega$ is in every single event except for perhaps a finite number
in $A_{i < j}$. Reverse. Let $j$ be the index of the largest event that
$\omega$ is not in. Then $\omega \in C_{n > j}$ and certainly $\omega \in
\cup^{\infty}C_n$.}
    \end{enumerate}
\end{proof}

\begin{note}
    The key idea above is this notion of "infinitely often" (i.o.) and "all but finitely
    often" (eventually) which are two distinct structures of infinite occurence in
    sequences. Consider an $\omega$ that exists in every other event (eg. just the
    odd indices) for infinite events and revisit its inclusion in $\cap^{\infty}
    B_i$ and $\cup^{\infty} C_i$.
\end{note}

\begin{note}
    % https://chatgpt.com/c/67c5f3d7-48f4-8004-a28f-8f25f47b44c4
    $\lim \cap \cup A_n$ is also referred to as the limit infimum of $A_n$. Similarly,
    $\lim \cup \cap A_n$ is referred to as the limit supremum of $A_n$.
    %TODO: show equivalence
    % Then revisit Borel Cantelli lemma: https://en.wikipedia.org/wiki/Borel%E2%80%93Cantelli_lemma
\end{note}

\setcounter{exercise}{6}
\begin{exercise}
    Let $\P(\bigcup\limits^n A_i) \leq \sum\limits^n \P(A_i)$. Then
    $\P(A_{n+1} \cup (\bigcup\limits^n A_i)) \leq \P(A_{n+1}) +
    (\sum\limits^n \P(A_i)) - \P(A_{n+1} \cap(\bigcup\limits^n A_i)) \leq
    \sum\limits^{n+1}\P(A_i)$
\end{exercise}

\begin{note}
% https://en.wikipedia.org/wiki/Boole%27s_inequality
    Expand a bit on the Boole inequality.
\end{note}

\setcounter{exercise}{8}
\begin{exercise}
    For fixed $B$ s.t. $\P(B) > 0$, show $\P(.\mid B)$ satisfies the three
    axioms of probability.
\end{exercise}
\begin{proof}
    \begin{itemize}
        \item{\textbf{Non-negativity}. If $\P(B) > 0$ and $\P(AB) > 0$ for any
                $A \subset \Omega$, certainly
            $\frac{\P(AB)}{\P(B)} > 0$}.
        \item{\textbf{Normalization}. $\frac{\P(\Omega \cap B)}{\P(B)} =
            \frac{\P(B)}{\P(B)} = 1$}
        \item{\textbf{Additivity}. Let $AB \cap CB = \emptyset$, then $\P(AB
            \cap CB) = \P(AB) + \P(CB)$. Indeed $\frac{\P(AB\cap CB)}{B} =
        \frac{\P(AB)}{B} + \frac{\P(CB)}{B}$}
    \end{itemize}
\end{proof}

\setcounter{exercise}{10}
\begin{exercise}
    Suppose $A$ and $B$ are independent events. Show that $A^c$ and $B^c$ are
    also independent.
\end{exercise}
\begin{proof}
    We are given $\P(AB) = \P(A)\P(B)$. Then $\P(A^c)\P(B^c) = (1 - \P(A))(1
    - \P(B)) = 1 - \P(A) - \P(B) + \P(A)\P(B) = 1 - \P(A\cup B) = \P(A^cB^c)$.
    The second to last equality uses independence of $P(AB)$. The last equality
    uses the property of set complements $P(A \cup B) = P(A^c \cap B^c)$.
\end{proof}

\setcounter{exercise}{12}
\begin{exercise}
    Suppose a fair coin is tossed repeatedly until heads and tails is each
    encoutered exactly once. Describe $\Omega$ and compute the probablity
    exactly three tosses are needed.
\end{exercise}
\begin{proof}
    \begin{itemize}
        \item{The sample space is the set of binary strings with exactly one
            $0$ and $1$. For strings of length greater than 2, these are
        repeated strings of $0$ or $1$ capped with a $1$ or $0$ respectively.}
    \item{By independence, each n-string has an identical probability
        $\frac{1}{2}^n$. There are two such 3-strings: 001 and 110. Using additivity,
    $\P(\text{3 tosses}) = \frac{1}{8} + \frac{1}{8} = \frac{1}{4}$}
    \end{itemize}
\end{proof}

\setcounter{exercise}{14}
\begin{exercise}
    The probability a child has blue eyes is $\frac{1}{4}$. Assume independence
    between children. Consider a family with 3 children.
    \begin{itemize}
        \item{If it is known that at least one of the children have blue eyes,
            what is the probablity that at least two of the children have blue
        eyes?}
    \item{If it is know that the youngest child has blue eyes, what is the
        probability that at least two of the children have blue eyes?}
    \end{itemize}
\end{exercise}
\begin{proof}
    \begin{itemize}
        \item{Straightforward conditional probability. Let $A$ be the event
                where at least one child has blue eyes and $B$ be the event
                where at least two children have blue eyes. Consider first,
                $\P(A) = 1 - \P(\text{no child has blue eyes}) =
                1 - \frac{27}{64} = \frac{37}{64}$. Compute $\P(A\cap B)$ by
                enumerating events 101, 111, 110 and using additivity: $2\cdot
                \frac{1}{4}^2\cdot\frac{3}{4} + \frac{1}{4}^3 = \frac{10}{64}$. Then $\P(B\mid A) = \frac{\P(A
            \cap B)}{\P(A)} = \frac{10}{64}\cdot\frac{64}{37} = \frac{10}{37}$}
        \item{Similar procedure. Let $A$ be the event where the youngest child
            has blue eyes and $B$ be as before. Using independence, $\P(A) =
        \frac{1}{4}$. (To see this rigorously, enumerate the sample space and
    see $\P(\Omega\mid \text{first child blue}) = 1$). Now $\P(B \cap A)$
    describe events 110, 101, 111 only. $\frac{7}{64}\cdot\frac{4}{1} =
\frac{7}{16}$.}
    \end{itemize}
\end{proof}

\setcounter{exercise}{16}
\begin{exercise}
    Show $\P(ABC) = \P(A \mid BC)\P(B \mid C)\P(C)$
\end{exercise}
\begin{proof}
    By straightforward application of the definition of conditional
    probability: $\dfrac{\P(ABC)}{\P(BC)}\dfrac{\P(BC)}{\P(C)}\P(C) = \P(ABC)$
\end{proof}

\begin{theorem}[Total Probability]
    If $A_1 \cdots A_k$ partition $\Omega$, $\P(B) = \sum_{i=1}^k\P(B\mid A_i)\P(A_i)$
\end{theorem}


\setcounter{exercise}{18}
\begin{exercise}
    Suppose $50\%$ of computer users are Windows. $30\%$ are Mac. $20\%$ are
    Linux. Suppose $65\%$ of Mac users, $82\%$ of
    Windows users and $50\%$ of Linux users get a virus. We select a person at random and learn
    they have the virus. What is the probability they are a Windows user?
\end{exercise}
\begin{proof}

    Let each $\omega \in \Omega$ be a distinct user. Then $W, M, L \subset
    \Omega$ are the users with Windows, Mac + Linux machines. $V, N \subset
    \Omega$ are the users with and without viruses.

    We want $\P(W\mid V) = \dfrac{\P(V\mid W)\P(W)}{\P(V)}$. Compute
$\P(V) = \sum_{X = \{W, M, L\}} \P(V\mid X)\P(X) = 0.705$. Then $\P(W\mid
V)= \dfrac{0.82\cdot0.50}{0.705} = 0.581$.
\end{proof}

\setcounter{exercise}{19}
\begin{exercise}

    A box contains 5 coins, each with a different probability of heads: $0,
    0.25, 0.5, 0.75, 1$. Let $C_i$ be the event with coin $i$ and $H_i$ be the
    event that heads is recovered on toss $i$.Suppose you select a coin at random and flip it.

    \begin{itemize}
    \item{What is the posterior probability $\P(C_i \mid H_1)$ for each coin?}
    \item{What is $\P(H_2 \mid H_1)$?}
    \item{Let $B_i$ be the event that the first heads is recovered on flip $i$.
        What is $\P(C_i \mid B_i) $ for each coin?}
    \end{itemize}
\end{exercise}

\begin{proof}
    \begin{itemize}
        \item{$\P(H_1) = \frac{1}{2}$. For each coin,
            $\P(C_i \mid H) = \dfrac{\P(H \mid C_i)\P(C_i)}{\P(H)}$. $\P(H)$
            can be worked out using total probability: $\sum_i \P(H |
        C_i)\P(C_i) = \frac{1}{2}$. Then eg. the posterior $\P(C_4\mid H) =
    \frac{3}{4}\cdot\frac{1}{5}\cdot\frac{2}{1} = \frac{3}{10}$.}
        \item{Note that both tosses are conditionally independent:
            $\P(H_2H_1\mid C_i) = \P(H_2\mid C_i)\P(H_1\mid C_i)$. $\P(H_2\mid
        H_1) = \dfrac{\P(H_2 H_1)}{\P(H_1)} = \dfrac{\sum_i \P(H_2 H_1 \mid C_i) \P(C_i)}{\sum_i \P(H_1 \mid C_i) \P(C_i)}.$ Because $\P(C_i)$ is
    uniform, we can simply to $\dfrac{\sum_i \P(H_2 H_1 \mid C_i)}{\sum_i \P(H_1 \mid C_i)}$. The result is $\dfrac{\sum_i p_i^2}{\sum_i p_i}$.}
    \item{Similar idea to (a).}
    \end{itemize}
\end{proof}

\begin{note}
    Important to see that independent events are not conditionally independent
    in general. Try to construct an example.
\end{note}


\subsection{Random Variables}

\begin{definition}[random variable]
\end{definition}


\begin{definition}[cumulative distribution function]
\end{definition}

The CDF contains "all the information" in a random variable. This is
articulated by the following theorem:

\begin{theorem}
    For random variables $X$ and $Y$ with CDFs $F$ and $G$, if $F(x) = G(X)
    \forall x \in [0, 1]$, then $X = Y$ ($\P(X \in A) = \P(Y \in A)$ for each
    $A \subset \R$).
\end{theorem}

And the behavior of the CDF, including "all of its information" is uniquely
determined by just three properties:

\begin{theorem}
    A function $F: \R \to [0, 1]$ is a CDF iff it satisfies three properties:
    \begin{itemize}
        \item{Non-decreasing. $x_2 > x_1 \implies F(x_2) \geq F(x_1)$}
        \item{Normalization. $\lim_{y\to 0}F(y) = 0$ and $\lim_{y\to 1}F(y) = 1$}
        \item{Right-continuous. For any $x \in \R$, $F(x) = F^+(x)$ where
            $F^+(x) = \lim_{y \to x, y>x}F(y)$}
    \end{itemize}
\end{theorem}

\begin{proof}
    Starting with (iii) from the text, let $A = (-\infty, x]$ and $y_1, y_2, \dots$ be a sequence where
        $y_1 < y_2 \dots$ and $\lim_i y_i = x$. By the definition of the
        CDF, $F(y_i) = \P(A_i)$ and $F(x) = \P(A)$, where $\lim_i F(y_i)$ is equivalent to
        $\lim_{y\to x, y>x}F(y)$. Observe $\cap_i A_i = A$ so $\P(A) =
        \P(\cap_i A_i) = \lim_i \P(A_i) = \lim_i F(y_i) = F(x)$ as desired.

        To see (ii), $\lim_{y\to -\infty}F(y) = 0$, define a sequence $y_1, y_2
        \cdots$ where $y_1 > y_2 \cdots$ as before and $y_1 = y$. Let $A_i = (\infty, y_i]$.
        Then $\cap_i A_i = \emptyset$ and $\P(\cap_i A_i) = \P(\emptyset) = 0$.
        Indeed $\lim_{y \to -\infty}F(y) = \lim_{i}\P(A_i) = \P(\cap_i A_i) =
        0$. A similar argument shows the limit to the other direction.

        For (iii), if $x_2 > x_1$ then $P((-\infty, x_2]) \geq P((-\infty,
        x_1])$ and $F(x_2) \geq F(x_1)$.
\end{proof}

The interesting direction is the reverse: a function satisfying these
properties uniquely determines a probability function. It is difficult to show
in general. A concrete example is the Cantor function
(\href{https://en.wikipedia.org/wiki/Cantor_function}{Devil's staircase}) which
satisfies non-decreasing, normality and right-continuous properties but from
which is difficult to derive a measure that satisfies eg. countable additivity.

\begin{note}
    A deeper measure theory course will approach this problem by defining the
    probablity function on an algebra of subsets rather than on each subset
    directly. Refer to tools like \href{https://en.wikipedia.org/wiki/Carath%C3%A9odory%27s_extension_theorem}{Caratheodory's extension theorem}.
\end{note}

\end{document}
