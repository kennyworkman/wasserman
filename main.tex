\documentclass[10pt]{article}
\usepackage{kennyworkman}

\title{Wasserman: All of Statistics}
\author{Kenny Workman}
\date{\today}

\begin{document}

\maketitle

\section{Probability}

\subsection{Basics}

\begin{definition}
    The \textbf{sample space} $\Omega$ is the set of outcomes from an experiment. Each
    point is denoted $\omega$ and subsets, eg. $A \subset \Omega$ are called
    \textbf{events}.
\end{definition}

\begin{definition}[Axioms of Probability]
    A function $\P: \Omega \to \R$ that assigns a real number to each event $A
    \subset \Omega$ is called a \textbf{probability function} or
    \textbf{probability measure} if it satsifies these three axioms:
    \begin{enumerate}
        \item \textbf{Non-negativity}. $\P(A) \ge 0$ for every event $A$
        \item \textbf{Normalization}. $\P(\Omega) = 1$.
        \item \textbf{Additivity}. $\P(A\cup B) = \P(A) + \P(B)$ if $A \cap B = \emptyset$.
    \end{enumerate}
\end{definition}

It is incredible, and not obvious, that much of probability is built up from
these only these three axioms

\begin{example}

It's actually tricky to show $\P(A \cup B) = \P(A) + \P(B) - \P(A \cap B)$ with
these three facts:

\begin{align*}
\P(A \cup B) &= \P(AB^c \cap AB \cap A^cB) \\
&= \P(AB^c) + \P(AB) + \P(A^cB) \\
&= \P(AB^c) + \P(AB) + \P(A^cB) + P(AB) - P(AB) \\
&= \P(AB^c \cup AB) + \P(A^cB \cup AB) - P(AB) \\
&= \P(A) + \P(B) - P(AB)
\end{align*}
\end{example}

Another simple idea is that events that are identical at the limit should have
identical probabilities.

\begin{theorem}[Continuity of Events]
If $A_n \to A$ then $\P(A_n) \to \P(A)$.
\end{theorem}

\begin{proof}
    Let $A_n$ be monotone increasing: $A_1 \subset A_2 \subset \dots$. Let
    $A = \lim_{n \to \infty} A_n = \bigcup_{i=1}^{\infty} A_i$.

    Construct disjoint sets $B_i$ from each $A_i$ where $B_1 = A_1$ and $B_n =
    \{ \omega \in \Omega : \omega \in A_n,\omega \notin \bigcup_{i=1}^{i-1}A_i \}$. It will be
    shown that (1) each pair of $B_i$ are disjoint, (2) $\bigcup_{i=1}^n A_i =
    \bigcup_{i=1}^n B_i$ and (3) $A = \bigcup_{i=1}^\infty A_i =
    \bigcup_{i=1}^\infty B_i$ (Exercise 1.1).

    From Axiom 3: $\P(A_n) = \P(\bigcup\limits_{i=1}^n A_i) = \P(\bigcup\limits_{i=1}^n B_i) =
    \sum\limits_{i=1}^n \P(B_i)$.

    Then $\lim_{n \to \infty} \P(A_n)
    = \lim_{n \to \infty} \sum\limits^{n}\P(B_n)
    = \sum\limits^{\infty}\P(B_n)
    = \P(\bigcup\limits^{\infty}B_n)
    = \P(A)$

\end{proof}

\begin{definition}[Conditional Probability]
    If $\P(B) > 0$, then the probability of $A$ given $B$ is 
    \[\P(A \mid B) = \dfrac{\P(AB)}{\P(B)}.\]
\end{definition}

\begin{theorem}[Total Probability]
    If $A_1 \cdots A_k$ partition $\Omega$, $\P(B) = \sum_{i=1}^k\P(B\mid A_i)\P(A_i)$
\end{theorem}

\begin{note}
    It can be difficult to assign a probability to every subset of $\Omega$. In
    practice, we only assign values to select subsets described by a \textbf{sigma
    algebra} denoted $A$. This is a subset algebra with three properties:

    \begin{itemize}
        \item{Non empty. $\emptyset \in A$. "Measure of the impossible."}
        \item{Closed over unions. $A_1, A_2 \cdots \in A \implies \cup_i A_i \in A$.}
        \item{Closed over complements. $A \in A \implies A^c \in A$.}
    \end{itemize}
    Every set in $A$ is considered \textbf{measurable} (by its membership in
    the sigma algebra). $A$ along with $\Omega$ comprises a \textbf{measurable
    space}, denoted by the pair $(A, \Omega)$. If the measure on $A$ is a
    probability function, importantly $\P(\Omega) = 1$, this space is also a
    \textbf{probability space}, denoted by the triple $(\Omega, A, \P)$.

    When $\Omega$ is the real line, the measure is often the Lebesgue measure,
    assigning intuitive values of "set length", eg. $[a, b] \mapsto b - a$.

    Why is this important? While overly pedantic at first glance, this is the
    structure that explains why continuous density functions (next section)
    have nonzero probabilities when integrated over intervals but assign 0
    probability to single points. The continuous measure, eg. Lebesgue measure,
    defined on the underlying probability space assigns positive values to sets
    and 0 to single points.
\end{note}


\begin{exercise}
    Fill in the details for Theorem 1.2 and extend to the case where $A_n$ is monotone decreasing.
\end{exercise}

\begin{proof}
    For any pair $B_{n+1}$ and $B_n$, because $B_n \subset A_n$ and $B_{n+1}
    \cap A_n = \emptyset$, it follows that $B_{n+1} \cap B_n = \emptyset$.

    Let $\bigcup_{i=1}^n B_i = \bigcup_{i=1}^n A_i$. Then $\bigcup_{i=1}^{n+1}
    B_i = (A_{n+1} \setminus \bigcup_{i=1}^n A_i) \bigcup (\bigcup_{i=1}^n
    A_i) = \bigcup_{i=1}^{n+1} A_i$.

    For the monotone decreasing case, let $A_n$ be a sequence where $A_1 \supset A_2 \supset A_3 \dots$. 

    Observe $A_1^c \subset A_2^c \dots$ and $\lim_{n \to \infty} A_n = \Omega
    \setminus \bigcup^{\infty} A_i^c$. Construct disjoint $B^c_n$ from $A^c$ in
    the same way.

    Then $\lim_{n \to \infty} \P(A_n) = 1 - \sum\limits^{\infty} \P(B^c_i) = 1 -
    \P(A^c) = \P(A)$
\end{proof}


\setcounter{exercise}{2}
\begin{exercise}
    Let $\Omega$ be a sample space and $A_1, A_2, \dots$ be events. Define
    $B_n = \cup_{i=n}^{\infty} A_i$ and $C_n = \cap_{i=n}^{\infty} A_i$.
    \begin{enumerate}[(a)]
        \item{Show $B_1 \supset B_2 \supset B_3 \dots$ and $C_1 \subset C_2
            \subset C_3 \dots$}
        \item{Show $\omega \in \cap_{n=1}^{\infty} B_n$ iff $\omega$ is in an
            infinite number of the events}
        \item{Show $\omega \in \cup{n=1}^{\infty} C_n$ iff $\omega$ belongs to all
                of the events, except possibly a finite number of those
            events.}
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[(a)]
        \item{Certainly $\cup_{i=1}^{\infty} A_i \supset \cup_{i=2}^{\infty} A_i \dots$ and $\cap_{i=1}^{\infty} A_i \subset \cap_{i=2}^{\infty} \dots$.}
        \item{Forward. Assume $\omega \in \cap_{n=1}^{\infty}B_n$. If $\omega$
            does not belong to an infinite number of events $A_i$, there exists
        some index $j$ past which $\omega \notin B_j$. Then certainly $\omega \notin \cap_{n=1}^{\infty}B_n$. Reverse. $\omega$ belonging to infinite events means there cannot exist such a $j$ described previously so $\omega \in B_n$ for all $n$. Indeed $\omega \in \cap_{n=1}^{\infty}B_n$}
    \item{Forward. Assume $\omega \in \cup_{n=1}^{\infty}C_n$. Then $\omega \in
        C_j = \cap_{i=j}^{\infty} A_i$ for some $j$. This is another way of
    saying $\omega$ is in every single event except for perhaps a finite number
in $A_{i < j}$. Reverse. Let $j$ be the index of the largest event that
$\omega$ is not in. Then $\omega \in C_{n > j}$ and certainly $\omega \in
\cup^{\infty}C_n$.}
    \end{enumerate}
\end{proof}

\begin{note}
    The key idea above is this notion of "infinitely often" (i.o.) and "all but finitely
    often" (eventually) which are two distinct structures of infinite occurence in
    sequences. Consider an $\omega$ that exists in every other event (eg. just the
    odd indices) for infinite events and revisit its inclusion in $\cap^{\infty}
    B_i$ and $\cup^{\infty} C_i$.
\end{note}

\begin{note}
    % https://chatgpt.com/c/67c5f3d7-48f4-8004-a28f-8f25f47b44c4
    $\lim \cap \cup A_n$ is also referred to as the limit infimum of $A_n$. Similarly,
    $\lim \cup \cap A_n$ is referred to as the limit supremum of $A_n$.
    %TODO: show equivalence
    % Then revisit Borel Cantelli lemma: https://en.wikipedia.org/wiki/Borel%E2%80%93Cantelli_lemma
\end{note}

\setcounter{exercise}{6}
\begin{exercise}
    Let $\P(\bigcup\limits^n A_i) \leq \sum\limits^n \P(A_i)$. Then
    $\P(A_{n+1} \cup (\bigcup\limits^n A_i)) \leq \P(A_{n+1}) +
    (\sum\limits^n \P(A_i)) - \P(A_{n+1} \cap(\bigcup\limits^n A_i)) \leq
    \sum\limits^{n+1}\P(A_i)$
\end{exercise}

\begin{note}
% https://en.wikipedia.org/wiki/Boole%27s_inequality
    Expand a bit on the Boole inequality.
\end{note}

\setcounter{exercise}{8}
\begin{exercise}
    For fixed $B$ s.t. $\P(B) > 0$, show $\P(.\mid B)$ satisfies the three
    axioms of probability.
\end{exercise}
\begin{proof}
    \begin{itemize}
        \item{\textbf{Non-negativity}. If $\P(B) > 0$ and $\P(AB) > 0$ for any
                $A \subset \Omega$, certainly
            $\frac{\P(AB)}{\P(B)} > 0$}.
        \item{\textbf{Normalization}. $\frac{\P(\Omega \cap B)}{\P(B)} =
            \frac{\P(B)}{\P(B)} = 1$}
        \item{\textbf{Additivity}. Let $AB \cap CB = \emptyset$, then $\P(AB
            \cap CB) = \P(AB) + \P(CB)$. Indeed $\frac{\P(AB\cap CB)}{B} =
        \frac{\P(AB)}{B} + \frac{\P(CB)}{B}$}
    \end{itemize}
\end{proof}

\setcounter{exercise}{10}
\begin{exercise}
    Suppose $A$ and $B$ are independent events. Show that $A^c$ and $B^c$ are
    also independent.
\end{exercise}
\begin{proof}
    We are given $\P(AB) = \P(A)\P(B)$. Then $\P(A^c)\P(B^c) = (1 - \P(A))(1
    - \P(B)) = 1 - \P(A) - \P(B) + \P(A)\P(B) = 1 - \P(A\cup B) = \P(A^cB^c)$.
    The second to last equality uses independence of $P(AB)$. The last equality
    uses the property of set complements $P(A \cup B) = P(A^c \cap B^c)$.
\end{proof}

\setcounter{exercise}{12}
\begin{exercise}
    Suppose a fair coin is tossed repeatedly until heads and tails is each
    encoutered exactly once. Describe $\Omega$ and compute the probablity
    exactly three tosses are needed.
\end{exercise}
\begin{proof}
    \begin{itemize}
        \item{The sample space is the set of binary strings with exactly one
            $0$ and $1$. For strings of length greater than 2, these are
        repeated strings of $0$ or $1$ capped with a $1$ or $0$ respectively.}
    \item{By independence, each n-string has an identical probability
        $\frac{1}{2}^n$. There are two such 3-strings: 001 and 110. Using additivity,
    $\P(\text{3 tosses}) = \frac{1}{8} + \frac{1}{8} = \frac{1}{4}$}
    \end{itemize}
\end{proof}

\setcounter{exercise}{14}
\begin{exercise}
    The probability a child has blue eyes is $\frac{1}{4}$. Assume independence
    between children. Consider a family with 3 children.
    \begin{itemize}
        \item{If it is known that at least one of the children have blue eyes,
            what is the probablity that at least two of the children have blue
        eyes?}
    \item{If it is know that the youngest child has blue eyes, what is the
        probability that at least two of the children have blue eyes?}
    \end{itemize}
\end{exercise}
\begin{proof}
    \begin{itemize}
        \item{Straightforward conditional probability. Let $A$ be the event
                where at least one child has blue eyes and $B$ be the event
                where at least two children have blue eyes. Consider first,
                $\P(A) = 1 - \P(\text{no child has blue eyes}) =
                1 - \frac{27}{64} = \frac{37}{64}$. Compute $\P(A\cap B)$ by
                enumerating events 101, 111, 110 and using additivity: $2\cdot
                \frac{1}{4}^2\cdot\frac{3}{4} + \frac{1}{4}^3 = \frac{10}{64}$. Then $\P(B\mid A) = \frac{\P(A
            \cap B)}{\P(A)} = \frac{10}{64}\cdot\frac{64}{37} = \frac{10}{37}$}
        \item{Similar procedure. Let $A$ be the event where the youngest child
            has blue eyes and $B$ be as before. Using independence, $\P(A) =
        \frac{1}{4}$. (To see this rigorously, enumerate the sample space and
    see $\P(\Omega\mid \text{first child blue}) = 1$). Now $\P(B \cap A)$
    describe events 110, 101, 111 only. $\frac{7}{64}\cdot\frac{4}{1} =
\frac{7}{16}$.}
    \end{itemize}
\end{proof}

\setcounter{exercise}{16}
\begin{exercise}
    Show $\P(ABC) = \P(A \mid BC)\P(B \mid C)\P(C)$
\end{exercise}
\begin{proof}
    By straightforward application of the definition of conditional
    probability: $\dfrac{\P(ABC)}{\P(BC)}\dfrac{\P(BC)}{\P(C)}\P(C) = \P(ABC)$
\end{proof}


\setcounter{exercise}{18}
\begin{exercise}
    Suppose $50\%$ of computer users are Windows. $30\%$ are Mac. $20\%$ are
    Linux. Suppose $65\%$ of Mac users, $82\%$ of
    Windows users and $50\%$ of Linux users get a virus. We select a person at random and learn
    they have the virus. What is the probability they are a Windows user?
\end{exercise}
\begin{proof}

    Let each $\omega \in \Omega$ be a distinct user. Then $W, M, L \subset
    \Omega$ are the users with Windows, Mac + Linux machines. $V, N \subset
    \Omega$ are the users with and without viruses.

    We want $\P(W\mid V) = \dfrac{\P(V\mid W)\P(W)}{\P(V)}$. Compute
$\P(V) = \sum_{X = \{W, M, L\}} \P(V\mid X)\P(X) = 0.705$. Then $\P(W\mid
V)= \dfrac{0.82\cdot0.50}{0.705} = 0.581$.
\end{proof}

\setcounter{exercise}{19}
\begin{exercise}

    A box contains 5 coins, each with a different probability of heads: $0,
    0.25, 0.5, 0.75, 1$. Let $C_i$ be the event with coin $i$ and $H_i$ be the
    event that heads is recovered on toss $i$.Suppose you select a coin at random and flip it.

    \begin{itemize}
    \item{What is the posterior probability $\P(C_i \mid H_1)$ for each coin?}
    \item{What is $\P(H_2 \mid H_1)$?}
    \item{Let $B_i$ be the event that the first heads is recovered on flip $i$.
        What is $\P(C_i \mid B_i) $ for each coin?}
    \end{itemize}
\end{exercise}

\begin{proof}
    \begin{itemize}
        \item{$\P(H_1) = \frac{1}{2}$. For each coin,
            $\P(C_i \mid H) = \dfrac{\P(H \mid C_i)\P(C_i)}{\P(H)}$. $\P(H)$
            can be worked out using total probability: $\sum_i \P(H |
        C_i)\P(C_i) = \frac{1}{2}$. Then eg. the posterior $\P(C_4\mid H) =
    \frac{3}{4}\cdot\frac{1}{5}\cdot\frac{2}{1} = \frac{3}{10}$.}
        \item{Note that both tosses are conditionally independent:
            $\P(H_2H_1\mid C_i) = \P(H_2\mid C_i)\P(H_1\mid C_i)$. $\P(H_2\mid
        H_1) = \dfrac{\P(H_2 H_1)}{\P(H_1)} = \dfrac{\sum_i \P(H_2 H_1 \mid C_i) \P(C_i)}{\sum_i \P(H_1 \mid C_i) \P(C_i)}.$ Because $\P(C_i)$ is
    uniform, we can simply to $\dfrac{\sum_i \P(H_2 H_1 \mid C_i)}{\sum_i \P(H_1 \mid C_i)}$. The result is $\dfrac{\sum_i p_i^2}{\sum_i p_i}$.}
    \item{Similar idea to (a).}
    \end{itemize}
\end{proof}

\begin{note}
    Important to see that independent events are not conditionally independent
    in general. Try to construct an example.
\end{note}


\subsection{Random Variables}
\renewcommand{\theexercise}{2.\arabic{exercise}}
\renewcommand{\thedefinition}{2.\arabic{definition}}
-- (kenny) TODO fix counters

\begin{definition}[Random Variable]
    A random variable $X$ is a function mapping the sample space to real
    numbers: $X: \Omega \to \R$.
\end{definition}

It is important to think of the relationship between the random variable and
its underlying sample space when computing probabilities: eg. $\P(X = x) =
\P(X^{-1}(x))$ and $\P(X \in A) = \P(X^{-1}(A))$.

\begin{definition}[Cumulative Distribution Function]
    The CDF is the function $F_X: \R \to [0, 1]$ where $F_X(x) = \P(X \leq x)$.
    Equivalently $F_X(x) = \P(X^{-1}((-\infty, x])$.
\end{definition}

The CDF contains "all the information" in a random variable. This is
articulated by the following theorem:

\begin{theorem}
    For random variables $X$ and $Y$ with CDFs $F$ and $G$, if $F(x) = G(X)
    \forall x \in [0, 1]$, then $X = Y$ ($\P(X \in A) = \P(Y \in A)$ for each
    $A \subset \R$).
\end{theorem}

And the behavior of the CDF, including "all of its information" is uniquely
determined by just three properties:

\begin{theorem}
    A function $F: \R \to [0, 1]$ is a CDF iff it satisfies three properties:
    \begin{itemize}
        \item{Non-decreasing. $x_2 > x_1 \implies F(x_2) \geq F(x_1)$}
        \item{Normalization. $\lim_{y\to 0}F(y) = 0$ and $\lim_{y\to 1}F(y) = 1$}
        \item{Right-continuous. For any $x \in \R$, $F(x) = F^+(x)$ where
            $F^+(x) = \lim_{y \to x, y>x}F(y)$}
    \end{itemize}
\end{theorem}

\begin{proof}
    Starting with (iii) from the text, let $A = (-\infty, x]$ and $y_1, y_2, \dots$ be a sequence where
        $y_1 < y_2 \dots$ and $\lim_i y_i = x$. By the definition of the
        CDF, $F(y_i) = \P(A_i)$ and $F(x) = \P(A)$, where $\lim_i F(y_i)$ is equivalent to
        $\lim_{y\to x, y>x}F(y)$. Observe $\cap_i A_i = A$ so $\P(A) =
        \P(\cap_i A_i) = \lim_i \P(A_i) = \lim_i F(y_i) = F(x)$ as desired.

        To see (ii), $\lim_{y\to -\infty}F(y) = 0$, define a sequence $y_1, y_2
        \cdots$ where $y_1 > y_2 \cdots$ as before and $y_1 = y$. Let $A_i = (\infty, y_i]$.
        Then $\cap_i A_i = \emptyset$ and $\P(\cap_i A_i) = \P(\emptyset) = 0$.
        Indeed $\lim_{y \to -\infty}F(y) = \lim_{i}\P(A_i) = \P(\cap_i A_i) =
        0$. A similar argument shows the limit to the other direction.

        For (iii), if $x_2 > x_1$ then $P((-\infty, x_2]) \geq P((-\infty,
        x_1])$ and $F(x_2) \geq F(x_1)$.
\end{proof}

The interesting direction is the reverse: a function satisfying these
properties uniquely determines a probability function. It is difficult to show
in general. A concrete example is the Cantor function
(\href{https://en.wikipedia.org/wiki/Cantor_function}{Devil's staircase}) which
satisfies non-decreasing, normality and right-continuous properties but from
which is difficult to derive a measure that satisfies eg. countable additivity.

\begin{note}
    A deeper measure theory course will approach this problem by defining the
    probablity function on an algebra of subsets rather than on each subset
    directly. Refer to tools like \href{https://en.wikipedia.org/wiki/Carath%C3%A9odory%27s_extension_theorem}{Caratheodory's extension theorem}.
\end{note}

It is from these random variables that we build "distributions", essentially
functions $\R \to [0, 1]$ that obey the three probability axioms.

\begin{definition}
    If $X$ "takes" countably many values (eg. has a countable range) it is
    \textbf{discrete}. $f_X(x) = \P(X = x)$ is its \textbf{probability mass
    function} or PMF.
\end{definition}

\begin{definition}
    $X$ is \textbf{continuous} if it has some $f_X$ that obeys three properties:
    \begin{itemize}
        \item{$\int_{-\infty}^{\infty} f_X(x) \dd x = 1$}
        \item{$ \forall x \in \R: f_X(x) \geq 0$}
        \item{$\P(a < X < b) = \int_a^b f_X(x) \dd x$}
    \end{itemize}
    $f_X$ is called the \textbf{probability density function} or PDF.
    Additionally, $F_X(x) = \int_{-\infty}^x f_X(x) \dd x$ and $f_X(x) =
    F'_X(x)$ for all points $x$ where $F_X$ is differentiable.
\end{definition}

The formal relation between the density function and the sample space is a bit
tricky, especially when $X$ is continuous. In practice, we often just produce
a function and deal with it directly while assuming the underlying sample space
with a well defined measure is lurking around.

\begin{note}
    We learned the probability function is defined on a well-defined sample space by measuring events / sets. 
\end{note}

\begin{definition}
    The quartile function (or inverse CDF) is $F^{-1}(q) = \inf\{ x : q < F(x) \}$
\end{definition}

We call $F^{-1}(\frac{1/4})$ the first quartile, $F^{-1}(\frac{1/2})$ the
second quartile (or median), etc.

We will proceed with some important mass functions.

\begin{definition}[The Point Mass Distribution]
    If $X \sim \sigma_a$ (reads "$X$ has a point mass distribution at $a$"),
    $f_X(a) = 1$ while $f_X(x) = 0$ for all $x \neq a$.
    \[
    F_X(x) =
    \begin{cases}
    0, & x < a, \\
    1, & x \geq a
    \end{cases}
    \]
    \end{definition}

\begin{definition}[The Uniform Distribution]
    Suppose $X$ has a mass function:
    \[f(x) = 
    \begin{cases}
        \frac{1}{k}, & x \in \{1 \dots k\} \\
        0, & \text{o.w.}
    \end{cases}
    \]
    $X$ then has a uniform distribution on $\{1 \dots k \}$.
\end{definition}

\begin{definition}[The Bernoulli Distribution]
    If $X \sim Bernoulli(p)$, the PMF of $X$ is $f(x) = p^x(1-p)^{1-x}$ for $x
    \in \{0, 1\}$ and $p \in [0, 1]$.
\end{definition}

Here is the first instance of a parameterized random variable.

\begin{definition}[The Binomial Distribution]
    Binomial variables model the number of successful flips for $n$ identical
    trials with probability $p$ for each.
    We say $X \sim Binomial(n, p)$ with PMF:
    \[f(x) = \begin{cases}
        \binom{n}{x}p^x(1-p)^{n-x} & x \in \{1 \dots n\} \\
        0 & \text{o.w.}
    \end{cases}
    \]
\end{definition}

The following represent different ideas of unbounded "counting": trials until
success and trials in some interval of time.

\begin{definition}[The Geometric Distribution]
    Here we have the idea of flipping a coin until our first success.
    $X \sim Geometric(p)$ with PMF: $f(x) = (1-p)^{x-1}p$
\end{definition}

The probability value of each term is a geometric series. Indeed $p
\sum^{\infty}(1-p)^x = \frac{p}{1-(1-p)} = 1$.

\begin{definition}[The Poisson Distribution]
    If $X \sim Poisson(\lambda)$ with PMF $f(x) = e^{-\lambda}\frac{\lambda^x}{x!}$
\end{definition}

$\lambda$ can be thought of as some interval of time. $X$ then measures the
number of events in this interval: decaying particles or mRNA translation.

Similarly to the geometric distribution, each term in the poisson is a Taylor
polynomial, derived from the power series expansion of the exponential
function. Indeed $e^{-\lambda}\sum^{\infty}\frac{\lambda^x}{x!} = e^{-\lambda}e^{\lambda}
= 1$. 

\begin{note}
    For distributions that count trials in some interval - some time or
    number of trials - the sum of variables equals a single variable that
    accumulates the interval. 

    If $X_1 \sim Binomial(n_1, p)$ and $X_2 \sim Binomial(n_2, p)$, then $X_1 +
    X_2 \sim Binomial(n_1 + n_2, p)$.

    If $X_1 \sim Poisson(\lambda_1)$ and $X_2 \sim Poisson(\lambda_2)$, then $X_1 +
    X_2 \sim Poisson(\lambda_1 + \lambda_2)$.
\end{note}

\begin{note}
    Recall $\Omega$ really lurking around. Eg. let $X \sim Bernoulli$ and $\P(X
    = 1)$ is $\P(\omega \in [0, p]) = p$. 
\end{note}

For the continuous distributions, useful to think of integration.

\begin{definition}[The Continuous Uniform Distribution]
    If $X$ has a uniform distribution on the interval $[a, b]$ with PDF: \[f(x) = \begin{cases}
        \frac{1}{b-a} & x \in [a, b] \\
        0 & \text{o.w.}
        \end{cases}
    \]
    and CDF: \[F(x) = \begin{cases}
        0 & x < a \\
        \frac{x - a}{b - a} & x \in [a, b] \\
        1 & x \geq b \\
    \end{cases}
    \]
\end{definition}

\begin{definition}[The Normal (Gaussian) Distribution]
    $f(x) = \frac{1}{\sigma \sqrt{2\pi}}e^{-\frac{1}{2\sigma^2}(x-\mu)^2}$
\end{definition}

\begin{note}
    If $X \sim N(0, 1)$ we say that $X$ has a \textbf{standard Normal
    distribution}. We often denote $X$ as $Z$ with $\phi$ and $\Phi$ as the
    PDF and CDF.

    There is no closed form function for $\Phi$, so we use precomputed values
    from tables or rely on statistical programs. Calculations with Normal
    distributions then proceed by reexpressing $X$ as some function of $Z$ and
    using these values.
\end{note}

The following facts are essential when manipulating these variables:

\begin{itemize}
\item{If $X \sim N(\mu, \sigma)$, then $Z = \frac{X - \mu}{\sigma} \sim N(0, 1)$}
\item{If $Z \sim N(0, 1)$, then $X = \mu + \sigma Z \sim N(\mu, \sigma)$}
\item{If $X_i \sim N(\mu_i, \sigma_i)$ are independent, then $X = \sum_i X_i \sim N(\sum_i \mu, \sum_i \sigma)$}
\end{itemize}

\begin{definition}[The Exponential Distribution]
    If $X \sim Exp(\beta)$, then $f(x) = \frac{1}{\beta}e^{-\frac{x}{\beta}}$.
\end{definition}

Indeed $\int$

\begin{note}[The Gamma Function]
    We often want a continuous extension of the factorial to real
    arguments, where $\Gamma(x) = (x-1)!$ for $x \in \Z^+$. This is the
    \textbf{gamma function} and defined $\Gamma(x) = \int_0^{\infty}
    y^{x-1}e^{-y}dy$.

    Evaluating the integral for $\Gamma(1), \Gamma(2) \dots$ is a useful
    exercise to convince oneself of agreement with the factorial.

    For example, $\Gamma(3) = \int_0^{\infty} y^2e^{-y} dy$. Using integration
    by parts, this evaluates to $[-y^2e^{-y} - 2ye^{-y} - 2e^{-y}]^{\infty}_0$.
    Using L'Hopital's, the first two terms drop out and we are left with
    $\Gamma(3) = 2 = (3-1)!$ as desired.
\end{note}

Equipped with the gamma function, we can now develop the gamma distribution.

\begin{definition}[Gamma Distribution]
  Let $\alpha,\beta>0$. A continuous random variable $X$ is said to have a
  \emph{Gamma distribution} with shape parameter $\alpha$ and scale parameter
  $\beta$, denoted
  \[
    X \sim \Gamma(\alpha, \beta),
  \]
  if its probability density function is
  \[
    f_X(x)
    = \frac{1}{\beta^{\alpha}\,\Gamma(\alpha)} \; x^{\alpha - 1} \, e^{-\,x/\beta},
    \quad x \ge 0.
  \]

  If $X_i \sim \Gamma(\alpha_i, \beta)$ are independent, $\sum_i X_i =
  \Gamma(\sum \alpha_i, \beta)$.
\end{definition}

The exponential distribution is then just a special case of a gamma
distribution with $\alpha = 1$.

\begin{note}
The Gamma‚Äênormalization comes from evaluating
\[
  \int_{0}^{\infty} x^{\alpha-1} \,e^{-x/\beta}\,dx.
\]
We make the substitution
\[
  x = \beta\,t,\quad dx = \beta\,dt,
\]
so that
\[
  \int_{0}^{\infty} x^{\alpha-1} e^{-x/\beta}\,dx
  = \int_{0}^{\infty} (\beta t)^{\alpha-1}\,e^{-t}\,(\beta\,dt)
  = \beta^{\alpha-1}\,\beta
    \int_{0}^{\infty} t^{\alpha-1} e^{-t}\,dt
  = \beta^{\alpha}\,\Gamma(\alpha).
\]
Hence in the density
\[
  f(x) = \frac{1}{\beta^\alpha\,\Gamma(\alpha)}\;x^{\alpha-1}\,e^{-x/\beta}
\]
the factor \(\beta^\alpha\,\Gamma(\alpha)\) is exactly the normalizing constant
that makes \(\int_0^\infty f(x)\,dx=1\).  
\end{note}

%% TODO: Transformations of random variables. Why F'_X = f_X.

\begin{definition}[Independence of Random Variables]
    If $\P(X \in A, Y \in B) = \P(X \in A)\P(Y \in B)$, we say $X$ and $Y$ are
    independent, written $X \indep Y$.
\end{definition}

\begin{definition}[Transformation of Continuous R.V.]
    When $Y$ and $X$ are continuous.
    \begin{itemize}
        \item{Find $A_y = \{ x : r(x) \leq y \}$ for each $y \in R$ }
        \item{Then $F_Y(y) = \P(r(X) \leq y) = \int_{A_y} f_X dx$}
        \item{$f_Y = F'_Y$}
    \end{itemize}
\end{definition}

\setcounter{exercise}{0}
\begin{exercise}
    Show $\P(X = x) = F(x^+) - F(x^-)$
\end{exercise}

\begin{proof}
    The key here is to see $\lim_{z < x, z\to x}F(z) = \P(X \in \cup_i (\infty,
    z_i]) = \P(X < x)$ for some sequence $z_1, z_2, \cdots $ where $\lim_i z_i
    = x_i$.
    While $\lim_{y > x, y\to x}F(y) = \P(X \in \cap_i(\infty, y_i]) = \P(X \leq
    x)$.

    Pay attention to the behavior of converging sets and the boundary. In the
    right-continous case, the sequence is approaching the boundary $x$ from
    above and each sequence is closed on $x$. Therefore in the limit, they
    include $x$.

    In the left-continuous case, the sequence is approaching the boundary $x$
    from below and each sequence excludes $x$. Therefore in the limit, they
    exclude $x$.

    To conclude $F(x^+) - F(x^-) = \P(X \leq x ) - \P(X < x) = \P(X = x)$. Of
    course, if $X$ is continuous, $F(x^+) = F(x^-)$ and $\P(X = x) = 0$,
    showing once again that every real value has no probability mass.
\end{proof}

\setcounter{exercise}{3}
\begin{exercise}
    Let $X$ have density
    \[
    f_X(x) =
    \begin{cases}
    \frac{1}{4}, & 0 < x < 1, \\
    \frac{3}{8}, & 3 < x < 5
    \end{cases}
    \]
    \begin{item}
    \item{Find the CDF of $f_X$}
    \item{Let $Y = \frac{1}{X}$. Find $f_Y$.}
    \end{item}
\end{exercise}

\begin{proof}
    \begin{itemize}
    %% TODO: unsure if this is sufficient
    \item{$F_X(x) = \int_{-\infty}^x f_X \dd x$}
\end{itemize}
\end{proof}

\end{document}
